{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vacV4BIFI8l"
      },
      "source": [
        "!pip3 -qq install torch==1.1\n",
        "!pip -qq install nltk==3.2.5\n",
        "!pip -qq install gensim==3.6.0\n",
        "!pip -qq install bokeh==1.0.4\n",
        "\n",
        "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
        "!unzip quora.zip\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIFSTdJG95SZ"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbpWIAreB6ky"
      },
      "source": [
        "# Introduction to PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M0mMOadG8aZ"
      },
      "source": [
        "PyTorch is one of the most well-known frameworks for building neural networks (which is what we're gonna do in this course).\n",
        "\n",
        "The most obvious alternative is Tensorflow, but right now (at fall of 2018) it's much less user-friendly so we'll stick to pytorch.\n",
        "\n",
        "And come on, if you learn one of them, you'll be able to learn another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsScdJ7DLZCm"
      },
      "source": [
        "## Automatic Differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1bSmJbXOPbk"
      },
      "source": [
        "Let's start with one of the fundamental pytorch concepts - automatic differentiation (autograd)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY9FHLM-M4aW"
      },
      "source": [
        "### Computational Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkvCloDpNXdH"
      },
      "source": [
        "Computational graphs provide a very convenient way to represent functions and calculate their gradients.\n",
        "\n",
        "For instance,\n",
        "$$f = (x + y) \\cdot z$$\n",
        "\n",
        "Can be represented with this graph:  \n",
        "![graph](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/Circuit.png)  \n",
        "*From [Backpropagation, Intuitions - CS231n](http://cs231n.github.io/optimization-2/)*\n",
        "\n",
        "*The forward pass* computes value of the function (green numbers). It starts from the inputs (on the left) and applies the sequence of functions.\n",
        "\n",
        "*The backward pass* (or *back propagation*) is designed to compute gradients of the function. That is $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$ in our case. It starts from the output and applies *chain rule* to compute them.\n",
        "\n",
        "For instance, for $f = q \\cdot z$, we have $\\frac{\\partial f}{\\partial q} = z$ and $\\frac{\\partial f}{\\partial z} = q$.  \n",
        "For $q = x + y$, we have $\\frac{\\partial q}{\\partial x} = \\frac{\\partial q}{\\partial y} = 1$.  \n",
        "Finally, we can apply the chain rule: $\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} = z$.\n",
        "\n",
        "*If you had problems with understanding the stuff above (and even if didn't), check this great tutorial: [Backpropagation, Intuitions - CS231n](http://cs231n.github.io/optimization-2/).*\n",
        "\n",
        "Well, such calculations in pytorch are fairly simple. You just have to describe your function as a sequence of operations, like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw4ASRktLdO4"
      },
      "source": [
        "x = torch.tensor(-2., requires_grad=True)\n",
        "y = torch.tensor(5., requires_grad=True)\n",
        "z = torch.tensor(-4., requires_grad=True)\n",
        "\n",
        "q = x + y\n",
        "f = q * z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-78COM99N8YL"
      },
      "source": [
        "Call it with some arguments and then ask it like, \"Hey, calc your grads, please\". And the magic happens:\n",
        "\n",
        "![graph](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)  \n",
        "*From [github.com/pytorch/pytorch](https://github.com/pytorch/pytorch)*\n",
        "\n",
        "Pytorch builds graph and performs backward pass - all by itself.\n",
        "\n",
        "If you already know tensorflow, you'll see the main difference: graph is built dynamically, it hasn't be compiled and stuff.\n",
        "\n",
        "Basically, it means that you can debug your code much more easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FOlPMIQMfbq"
      },
      "source": [
        "f.backward()\n",
        "\n",
        "print('df/dz =', z.grad)\n",
        "print('df/dx =', x.grad)\n",
        "print('df/dy =', y.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JotDf1naGU-R"
      },
      "source": [
        "The call of the `backward()` method calculates gradients for all tensors in graph (except the subgraphs where `requires_grad == False`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSiB1CGyJMzt"
      },
      "source": [
        "*Read about autograd in pytorch in depth here: [Autograd mechanics](https://pytorch.org/docs/stable/notes/autograd.html).*\n",
        "\n",
        "Well, the nicest thing about pytorch is that you can use it like you used numpy. You use `ndarray` all the time, right.\n",
        "\n",
        "There is an analog for it - `tensor`. We just created few of them actually.\n",
        "\n",
        "It stores data, like `ndarray`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY2CcCw2Gmgq"
      },
      "source": [
        "x.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYxD8N_9GpJl"
      },
      "source": [
        "And gradient (unlike `ndarray`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYCD5P24GufX"
      },
      "source": [
        "x.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwLx4szvGwMb"
      },
      "source": [
        "Add function used to compute the gradient:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTfGdUF_GzV8"
      },
      "source": [
        "q.grad_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgK1Esa6HHAB"
      },
      "source": [
        "And lots of meta-info:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nazaer0AG4pL"
      },
      "source": [
        "x.type(), x.shape, x.device, x.layout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Be6fwAky0pr"
      },
      "source": [
        "Check this tutorial to learn more about tensors: [Deep Learning with PyTorch: A 60 Minute Blitz > What is PyTorch?](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)\n",
        "\n",
        "Sometimes we don't want to compute the gradients. To handle this cases (we'll discuss particular examples very soon), you can use context managers ([Locally disabling gradient computation](https://pytorch.org/docs/stable/autograd.html#locally-disabling-gradient-computation)):\n",
        "```python\n",
        "torch.autograd.no_grad()\n",
        "torch.autograd.enable_grad()\n",
        "torch.autograd.set_grad_enabled(mode)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQEJeqfnJPpA"
      },
      "source": [
        "with torch.autograd.no_grad():\n",
        "    x = torch.tensor(-2., requires_grad=True)\n",
        "    y = torch.tensor(5., requires_grad=True)\n",
        "    q = x + y\n",
        "\n",
        "z = torch.tensor(-4., requires_grad=True)\n",
        "f = q * z\n",
        "\n",
        "f.backward()\n",
        "\n",
        "print('df/dz =', z.grad)\n",
        "print('df/dx =', x.grad)\n",
        "print('df/dy =', y.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvLFlc4iQOQv"
      },
      "source": [
        "Well, the question is why on earth you would want it to use :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlhLBWwHG3Xe"
      },
      "source": [
        "### Warm-up Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaqtIIvJOEut"
      },
      "source": [
        "To understand it, let's write a simple linear regression implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDZpEHF8AKH2"
      },
      "source": [
        "w_orig, b_orig = 2.6, -0.4\n",
        "\n",
        "X = np.random.rand(100) * 10. - 5.\n",
        "y_orig = w_orig * X + b_orig\n",
        "\n",
        "y = y_orig + np.random.randn(100)\n",
        "\n",
        "plt.plot(X, y, '.')\n",
        "plt.plot(X, y_orig)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2K5MVtiSGuC"
      },
      "source": [
        "There are two parameters $w$ and $b$. We want them to be as near to $w_{orig}, b_{orig}$ as possible.\n",
        "\n",
        "What are we going to optimize? Let's optimize MSE loss:\n",
        "$$J(w, b) = \\frac{1}{N} \\sum_{i=1}^N || \\hat y_i - y_i(w, b)||^2 =\\frac{1}{N} \\sum_{i=1}^N || \\hat y_i - (w \\cdot x_i + b)||^2. $$\n",
        "\n",
        "We can use *gradient descent* algorithm to optimize it (not even stohastic right now):\n",
        "$$w_{t+1} := w_t - \\alpha \\cdot \\frac{\\partial J}{\\partial w}(w_t, b_t)$$\n",
        "$$b_{t+1} := w_t - \\alpha \\cdot \\frac{\\partial J}{\\partial b}(w_t, b_t)$$\n",
        "\n",
        "*You see, it would be nice to use backpropagation here.*\n",
        "\n",
        "**Task** Implement the optimization using pure numpy.\n",
        "\n",
        "You'll need:\n",
        "1. Perform the forward pass: $y(w, b) = w \\cdot x + b$;\n",
        "2. Find the gradients $\\frac{\\partial J}{\\partial w}, \\frac{\\partial J}{\\partial b}$ using backward pass;\n",
        "3. Move $w, b$ in the anti-gradients direction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKbqTNVXFB3A"
      },
      "source": [
        "def display_progress(epoch, loss, w, b, X, y, y_pred):\n",
        "    clear_output(True)\n",
        "    print('Epoch = {}, Loss = {}, w = {}, b = {}'.format(epoch, loss, w, b))\n",
        "    plt.plot(X, y, '.')\n",
        "    plt.plot(X, y_pred)\n",
        "    plt.show()\n",
        "    time.sleep(1)\n",
        "\n",
        "\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "\n",
        "alpha = 0.01\n",
        "\n",
        "for i in range(100):\n",
        "    <calculate model's output>\n",
        "\n",
        "    <calculate loss>\n",
        "\n",
        "    <calculate gradients>\n",
        "\n",
        "    <update w and b>\n",
        "\n",
        "    if (i + 1) % 5 == 0:\n",
        "        display_progress(i + 1, loss, w, b, X, y, y_pred)\n",
        "\n",
        "assert np.abs(w - w_orig) < 0.1, 'Something went wrong :('"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8WgWrF4C2WK"
      },
      "source": [
        "It's much simpler to implement the same thing using pytorch.\n",
        "\n",
        "The forward pass will look almost the same. And we've already learnt how to perform the backward pass! Just call `loss.backward()`.\n",
        "\n",
        "But there are a number of caveats you have to know about.\n",
        "\n",
        "First of all, one doen't simply update `w` and `b`. Try this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx4DoGeBMJd4"
      },
      "source": [
        "w = torch.randn(1, requires_grad=True)\n",
        "\n",
        "w -= 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OjoUh-SMPBt"
      },
      "source": [
        "It should fail with a message like:  \n",
        "`RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.`\n",
        "\n",
        "The issue is in the support of in-place operations in autograd: [In place operations with autograd](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd).\n",
        "\n",
        "But actually we are not going to perform an operation that requires gradients. We're just updating the value of the note.\n",
        "\n",
        "To fight this problem, we can either use `no_grad` context or update the underline data used by the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zegkKd-cMOMj"
      },
      "source": [
        "w.data -= 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVlaIdvHNXR_"
      },
      "source": [
        "Another thing you should be aware of is that the gradients are accumulating by default. So you have to update them yourself between `loss.backward()` calls:\n",
        "```python\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "```\n",
        "\n",
        "**Task** Implement the linear regression on pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRqxypuEU2ig"
      },
      "source": [
        "X = torch.as_tensor(X).float()\n",
        "y = torch.as_tensor(y).float()\n",
        "\n",
        "w = torch.randn(1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "for i in range(100):\n",
        "    <copy forward pass and add backward pass + parameters updates>\n",
        "\n",
        "    if (i + 1) % 5 == 0:\n",
        "        display_progress(i + 1, loss, w.item(), b.item(),\n",
        "                         X.data.numpy(), y.data.numpy(), y_pred.data.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaKTKN_fOvo-"
      },
      "source": [
        "Much simpler, isn't it? :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZNq6ujzPtvd"
      },
      "source": [
        "## Word Embeddings (via High-Level PyTorch API)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITLgcVz66AfV"
      },
      "source": [
        "Let's move now to more high-level API, where all the good neural network parts are implemented. Quite comprehensive description of it is given in this tutorial: [What is torch.nn really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n",
        "\n",
        "Last time we used gensim to train word2vec model. Now we're ready to implement our own model.\n",
        "\n",
        "Well, almost ready. We still haven't discussed what word2vec is.\n",
        "\n",
        "The key idea is simple: you can understand the meaning of the word by his neighbours (the words that appear frequently in its context):  \n",
        "![contexts](https://image.ibb.co/mnQ2uz/2018_09_17_21_07_08.png)\n",
        "*From [cs224n, Lecture 2](http://web.stanford.edu/class/cs224n/lectures/lecture2.pdf)*\n",
        "\n",
        "The first idea is just to use counts of the words in context as a meaningful word vector.\n",
        "\n",
        "For instance, for such simple corpus:\n",
        "\n",
        "```\n",
        "The red fox jumped\n",
        "The brown fox jumped\n",
        "```\n",
        "\n",
        "we'll have following count vectors:\n",
        "```\n",
        "        the fox jumped red brown\n",
        "red   = (1   1    1     0    0)\n",
        "brown = (1   1    1     0    0)\n",
        "```\n",
        "\n",
        "You see, `red` and `brown` have similar vector! The problem is almost solved. But we have to obtain much smaller embedding vectors.\n",
        "\n",
        "And here is what word2vec algorithms do. They build embedding vectors based on the neighbours of the word in corpus.\n",
        "\n",
        "A nice introduction is given in this post: [king - man + woman is queen; but why?](http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html)\n",
        "\n",
        "Let's do some preparation work before moving to the interesting stuff.\n",
        "\n",
        "**Task** Tokenize and lower-case texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKKb9Ya8hzIb"
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "quora_data = pd.read_csv('train.csv')\n",
        "\n",
        "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
        "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
        "\n",
        "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
        "\n",
        "tokenized_texts = [<do it there>]\n",
        "\n",
        "assert len(tokenized_texts) == len(texts)\n",
        "assert isinstance(tokenized_texts[0], list)\n",
        "assert isinstance(tokenized_texts[0][0], str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYoj91iDDDfT"
      },
      "source": [
        "Collect the indices of the most frequent words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PL471pGjuVN"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "MIN_COUNT = 5\n",
        "\n",
        "words_counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
        "word2index = {\n",
        "    '<unk>': 0\n",
        "}\n",
        "\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < MIN_COUNT:\n",
        "        break\n",
        "\n",
        "    word2index[word] = len(word2index)\n",
        "\n",
        "index2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]\n",
        "\n",
        "print('Vocabulary size:', len(word2index))\n",
        "print('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\n",
        "print('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\n",
        "print('Most freq words:', index2word[1:21])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF5mYpCsE9Uh"
      },
      "source": [
        "### Skip-Gram Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om1IG5XEMGRa"
      },
      "source": [
        "Word2vec is actually a set of models used to build word embeddings.\n",
        "\n",
        "We are going to start with the *skip-gram model*.\n",
        "\n",
        "It's a very simple neural network with just two layers. It aims to build word vectors that encode information about the co-occurring words:  \n",
        "![](https://i.ibb.co/nL0LLD2/Word2vec-Example.jpg)  \n",
        "*From cs224n, Lecture 2*\n",
        "\n",
        "More precisely, it models the probabilities $\\{P(w_{c+j}|w_c):  j = c-k, ..., c+k, j \\neq c\\}$, where $k$ is the context window size, $c$ is index of the central word (which embedding we are trying to optimize).\n",
        "\n",
        "The learnable parameters of the model are following: matrix $U$ (embeddings' matrix that is used in all downstream tasks. In gensim it's called `syn0`) and matrix $V$ - output layer of the model (in gensim it's called `syn1`).\n",
        "\n",
        "Two vectors correspond to each word: a row in $U$ and a column in $V$. That is $U \\in \\mathbb{R}^{|V|, d}$ and $V \\in \\mathbb{R}^{d, |V|}$, where $d$ is embedding size and $|V|$ is the vocabulary size.\n",
        "\n",
        "As a result, the neural network looks this way:  \n",
        "![skip-gram](https://i.ibb.co/F54XzDC/SkipGram.png)\n",
        "\n",
        "What's going on and how it is connected to probability and word context?\n",
        "\n",
        "Well, the word is mapped to its embedding $u_c$. Then this embedding is multiplied to matrix $V$.\n",
        "\n",
        "As a result, we obtain the set of scores $\\{v_j^T u_c : j \\in {0, \\ldots, |V|}\\}$. Each corresponds to the similarity between the word $w_j$ vector and our word vector. It's very similar to the cosine similarity we calculated in the previous lesson, but without normalization.\n",
        "\n",
        "This similarities show how likely $w_j$ can be in context of word $w_c$. That means, that they can be converted to probability using the softmax function:\n",
        "$$P(w_j | w_c) = \\frac{\\exp(v_{j}^T u_c)}{\\sum_{i=1}^{|V|} \\exp(v_i^T u_c)}.$$\n",
        "\n",
        "So for each word we calculate such probability distribution over our vocabulary. It's shown in using blue bars in the picture above. More likely word - bluer is the corresponding cell.\n",
        "\n",
        "The model learns to distribute the probabilities between the co-occuring words for the given one. We'll use cross-entropy loss for it:\n",
        "$$-\\sum_{-k \\leq j \\leq k, j \\neq 0} \\log \\frac{\\exp(v_{c+j}^T u_c)}{\\sum_{i=1}^{|V|} \\exp(v_i^T u_c)} \\to \\min_{U, V}.$$\n",
        "\n",
        "For instance, for the sample from the picture model will be penalized if it outputs a low probability of word `over`.\n",
        "\n",
        "Please, notice that we calculate the similarity between vectors from different vector spaces. $u_c$ is the vector from the input embeddings and $v_j$ is the vector from the output embeddings. A high similarity between them means that they co-occur frequently, not that they are similar in the syntactic role or their semantics.\n",
        "\n",
        "On the other hand, the similarity between $u_k$ and $u_m$ means that their output distributions are similar. And that means exactly that the similarity of the count vectors we discussed before and also most probably means their syntactic or semantic similarity.\n",
        "\n",
        "Check this demo to understand what's going on in more depth: [https://ronxin.github.io/wevi/](https://ronxin.github.io/wevi/).\n",
        "\n",
        "Let's implement it now!\n",
        "\n",
        "#### Batches Generations\n",
        "\n",
        "First of all, we need to collect all the contexts from our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocrsXgaynYPG"
      },
      "source": [
        "def build_contexts(tokenized_texts, window_size):\n",
        "    contexts = []\n",
        "    for tokens in tokenized_texts:\n",
        "        for i in range(len(tokens)):\n",
        "            central_word = tokens[i]\n",
        "            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1)\n",
        "                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n",
        "\n",
        "            contexts.append((central_word, context))\n",
        "\n",
        "    return contexts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQBa6yQ9BXjp"
      },
      "source": [
        "contexts = build_contexts(tokenized_texts, window_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o_ePiZ7wfpT"
      },
      "source": [
        "Check, what you got:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyQNK-9SBdb9"
      },
      "source": [
        "contexts[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbQKln_6yC4l"
      },
      "source": [
        "Let's convert words to indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOPRlKlLvUBA"
      },
      "source": [
        "contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context])\n",
        "            for central_word, context in contexts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYmrAi9gyIe-"
      },
      "source": [
        "Neural networks are optimized using stochastic gradient descent methods. Which means, we need a batch generator - a function that generates samples to optimize neural network with.\n",
        "\n",
        "A simple batch generator looks this way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC9SifFU5iQP"
      },
      "source": [
        "import random\n",
        "\n",
        "def make_skip_gram_batches_iter(contexts, window_size, num_skips, batch_size):\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * window_size\n",
        "\n",
        "    central_words = [word for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
        "    contexts = [context for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
        "\n",
        "    batch_size = int(batch_size / num_skips)\n",
        "    batches_count = int(math.ceil(len(contexts) / batch_size))\n",
        "\n",
        "    print('Initializing batches generator with {} batches per epoch'.format(batches_count))\n",
        "\n",
        "    while True:\n",
        "        indices = np.arange(len(contexts))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(batches_count):\n",
        "            batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
        "            batch_indices = indices[batch_begin: batch_end]\n",
        "\n",
        "            batch_data, batch_labels = [], []\n",
        "\n",
        "            for data_ind in batch_indices:\n",
        "                central_word, context = central_words[data_ind], contexts[data_ind]\n",
        "\n",
        "                words_to_use = random.sample(context, num_skips)\n",
        "                batch_data.extend([central_word] * num_skips)\n",
        "                batch_labels.extend(words_to_use)\n",
        "\n",
        "            yield {\n",
        "                'tokens': torch.cuda.LongTensor(batch_data),\n",
        "                'labels': torch.cuda.LongTensor(batch_labels)\n",
        "            }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmRGZ-vG5iQR"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_Yeowx15iQS"
      },
      "source": [
        "batch = next(make_skip_gram_batches_iter(contexts, window_size=2, num_skips=2, batch_size=32))\n",
        "\n",
        "batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DXjZS3JyQZh"
      },
      "source": [
        "#### nn.Sequential\n",
        "\n",
        "The simplest way to implement a model on pytorch is to use `nn.Sequential`. Just define the order of layers and it will apply them sequentially."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRw9Z4G__46O"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Embedding(len(word2index), 32),\n",
        "    nn.Linear(32, len(word2index))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysn0DDpLyj1S"
      },
      "source": [
        "Yep, we've just defined our skip-gram model!\n",
        "\n",
        "The construction above says that we need a `nn.Embedding` layer (a layer that maps index to a vector. In our case it would be an index from the `range(len(word2index))` and a 32-dimensional vector) following by a `nn.Linear` layer (just a dot-product of an input vector to the learnable matrix with addition of a learnable bias).\n",
        "\n",
        "There is another pytorch's feature we haven't discussed yet. It's computations on GPU. Neural networks are usually trained on GPUs because it's much faster.\n",
        "\n",
        "It's extremely easy to ask pytorch to perform calculations on the GPU. Just call:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfmaUi3Uy9YT"
      },
      "source": [
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3c3UEa2zHhk"
      },
      "source": [
        "or"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHxAg5ZWzEKT"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prP1avp-5DjZ"
      },
      "source": [
        "We'll apply it to the data in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsa-P9VS49RD"
      },
      "source": [
        "tokens, labels = batch['tokens'], batch['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtLMvOO2z3c8"
      },
      "source": [
        "Now, to perform the forward pass just call the model with its input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9wTpewTz3Dk"
      },
      "source": [
        "logits = model(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWJmDy_uzJgD"
      },
      "source": [
        "We can calculate the loss now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7rlD62_ykYl"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "loss = criterion(logits, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAwx-pck0RxX"
      },
      "source": [
        "And, of course, perform the backward pass:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWt6gL0_0Npp"
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJkDOl6szRLm"
      },
      "source": [
        "Finally, we have to update the parameters. We can do it as before (like, `w.data -= alpha * w.grad`). But pytorch contains predefined optimizers that can do the same things (and more complicated stuff).\n",
        "\n",
        "We are going to use Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-b5CIARzQ6m"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju5lO0Xi0hsV"
      },
      "source": [
        "To update the weights just call `optimizer.step()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9QK7nHu0Zw8"
      },
      "source": [
        "print(model[1].weight)\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print(model[1].weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnxyk1ew0pSk"
      },
      "source": [
        "And finally, don't forget to zero grads!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMsuvEP90svi"
      },
      "source": [
        "optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXSzfnLf3hDa"
      },
      "source": [
        "You can ask optimizer to do it for you, you see."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PibTw33Azg7q"
      },
      "source": [
        "#### Train Cycle\n",
        "\n",
        "We are ready to implement the training cycle - just like we did for our linear regression.\n",
        "\n",
        "**Task** Implement the cycle. Train the model for a few epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewGMgYTXANzz"
      },
      "source": [
        "loss_every_nsteps = 1000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for step, (batch, labels) in enumerate(make_skip_gram_batches_iter(contexts, window_size=2, num_skips=4, batch_size=128)):\n",
        "    <1. convert data to tensors>\n",
        "\n",
        "    <2. make forward pass>\n",
        "\n",
        "    <3. make backward pass>\n",
        "\n",
        "    <4. apply optimizer>\n",
        "\n",
        "    <5. zero grads>\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if step != 0 and step % loss_every_nsteps == 0:\n",
        "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps,\n",
        "                                                                    time.time() - start_time))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqq9kee41L4P"
      },
      "source": [
        "#### Analysis\n",
        "\n",
        "To get the embedding matrix, cast the following magic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWsYkNn-Hnl_"
      },
      "source": [
        "embeddings = model[0].weight.data.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fvE2za248_A"
      },
      "source": [
        "That is, we get the weights from the first layer of the model, move them from gpu to cpu and convert to numpy array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZtxY2D01RB6"
      },
      "source": [
        "Let's check how adequate are similarities that the model learnt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhDwuhDSHEDm"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(embeddings, index2word, word2index, word):\n",
        "    word_emb = embeddings[word2index[word]]\n",
        "\n",
        "    similarities = cosine_similarity([word_emb], embeddings)[0]\n",
        "    top10 = np.argsort(similarities)[-10:]\n",
        "\n",
        "    return [index2word[index] for index in reversed(top10)]\n",
        "\n",
        "most_similar(embeddings, index2word, word2index, 'warm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VS1x-mO1WKS"
      },
      "source": [
        "And visualizations!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuXv2HxsAecb"
      },
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "\n",
        "    if isinstance(color, str):\n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show:\n",
        "        pl.show(fig)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    tsne = TSNE(n_components=2, verbose=100)\n",
        "    return scale(tsne.fit_transform(word_vectors))\n",
        "\n",
        "\n",
        "def visualize_embeddings(embeddings, index2word, word_count):\n",
        "    word_vectors = embeddings[1: word_count + 1]\n",
        "    words = index2word[1: word_count + 1]\n",
        "\n",
        "    word_tsne = get_tsne_projection(word_vectors)\n",
        "    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)\n",
        "\n",
        "\n",
        "visualize_embeddings(embeddings, index2word, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGfhLR6x8D3r"
      },
      "source": [
        "### Continuous Bag of Words (CBoW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UuVr2IsaYhX"
      },
      "source": [
        "Here is an alternative word2vec model:\n",
        "\n",
        "![](https://i.ibb.co/StXTMFH/CBOW.png)\n",
        "\n",
        "Now we have to predict the word by its context. The context is represented as a sum of context vectors.\n",
        "\n",
        "**Task** Implement the batch generator. It should output a context matrix `(samples_count, 2 * window_size)` which contains the context word indices and a target matrix `(samples_count)` with the central word indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNaP0uaU7T2-"
      },
      "source": [
        "def make_cbow_batches_iter(contexts, window_size, batch_size):\n",
        "    data = np.array([context for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "    labels = np.array([word for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "\n",
        "    batches_count = int(math.ceil(len(data) / batch_size))\n",
        "\n",
        "    print('Initializing batches generator with {} batches per epoch'.format(batches_count))\n",
        "\n",
        "    while True:\n",
        "        indices = np.arange(len(data))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(batches_count):\n",
        "            <implement the generator>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBF7xiik7ZaN"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IVrQl8S4L9j"
      },
      "source": [
        "window_size = 2\n",
        "batch_size = 32\n",
        "\n",
        "batch = next(make_cbow_batches_iter(contexts, window_size=window_size, batch_size=batch_size))\n",
        "\n",
        "assert isinstance(batch, dict)\n",
        "assert 'labels' in batch and 'tokens' in batch\n",
        "\n",
        "assert isinstance(batch['tokens'], torch.cuda.LongTensor)\n",
        "assert isinstance(batch['labels'], torch.cuda.LongTensor)\n",
        "\n",
        "assert batch['tokens'].shape == (batch_size, 2 * window_size)\n",
        "assert batch['labels'].shape == (batch_size,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbKbZ_4E7T3U"
      },
      "source": [
        "The alternative way to define a model is to inherit it from `nn.Module`. It's a more flexible approach than defining a `nn.Sequential` model so we are going to use it mostly\n",
        "\n",
        "```python\n",
        "class MyNetModel(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(MyNetModel, self).__init__()\n",
        "        <initialize layers>\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        <apply layers>\n",
        "        return final_output\n",
        "```\n",
        "\n",
        "You have to create all the learnable parameters (usually - layers) of the model in the `__init__` method and you have to apply them in the `forward` method.\n",
        "\n",
        "**Task** Build the `CBoWModel`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkawxwe77T3V"
      },
      "source": [
        "class CBoWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self._embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self._out_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        <apply the layers>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhTDxya7a3S"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh_mNh__6lG2"
      },
      "source": [
        "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
        "\n",
        "outputs = model(batch['tokens'])\n",
        "\n",
        "assert isinstance(outputs, torch.cuda.FloatTensor)\n",
        "assert outputs.shape == (batch_size, len(word2index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmn56yki7T3a"
      },
      "source": [
        "**Task** Train the model in the same way as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzKLP9bs7T3b"
      },
      "source": [
        "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "loss_every_nsteps = 1000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for step, batch in enumerate(make_cbow_batches_iter(contexts, window_size=2, batch_size=128)):\n",
        "    <copy-paste the learning cycle>\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if step != 0 and step % loss_every_nsteps == 0:\n",
        "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps,\n",
        "                                                                    time.time() - start_time))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQW4PBdF96xC"
      },
      "source": [
        "Let's visualize what we got."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTEWcyYmvips"
      },
      "source": [
        "visualize_embeddings(model.embeddings.weight.data.cpu().numpy(), index2word, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CON4VOyG3iET"
      },
      "source": [
        "### Negative Sampling\n",
        "\n",
        "What is the most computationally hard part of the model optimization? It's computation of softmax function over the vocabulary.\n",
        "\n",
        "To improve the model's performance *negative sampling* can be used.\n",
        "\n",
        "The idea is fairly obvious: let's predict the probability that the word $w$ can be in the context $c$: $P(D=1|w,c)$.\n",
        "\n",
        "The probability (again!) would be a function of the similarity between vectors:\n",
        "$$P(D=1|w, c) = \\sigma(v_w^T u_c) = \\frac 1 {1 + \\exp(-v^T_w u_c)}.$$\n",
        "\n",
        "The sigmoid function just maps the similarity to [0, 1] range.\n",
        "\n",
        "Well, we have positive samples (word-context pairs from the corpus), but we need some negative samples too to train our model. And we can generate them!  \n",
        "![Negative Sampling](https://i.ibb.co/D7rTdbr/Negative-Sampling.png)\n",
        "\n",
        "Just sample random word instead of the correct one and hope that they don't fit the context too well.\n",
        "\n",
        "The loss function (in CBoW setup) will be following:\n",
        "$$-\\log \\sigma(v_c^T u_c) - \\sum_{k=1}^K \\log \\sigma(-\\tilde v_k^T u_c),$$\n",
        "where $v_c$ - central word vector, $u_c$ - context vector (sum of vectors from the context), $\\tilde v_1, \\ldots, \\tilde v_K$ - vectors of the negative samples.\n",
        "\n",
        "Compare it with ordinary CBoW:\n",
        "$$-v_c^T u_c + \\log \\sum_{i=1}^{|V|} \\exp(v_i^T u_c).$$\n",
        "\n",
        "You see, it's quite similar, but the sum is over a much smaller number of samples.\n",
        "\n",
        "The sampling is performed from $U^{3/4}$, where $U$ - unigram distribution (word frequencies).\n",
        "\n",
        "We've already calculated the word counts (they were obtained when we called `Counter(words)`).\n",
        "\n",
        "Just convert them to frequencies and raise them to the power of $\\frac 3 4$. Why exactly $\\frac 3 4$? It's just a good constant, but the intuition is following:\n",
        "\n",
        "$$P(\\text{is}) = 0.9, \\ P(\\text{is})^{3/4} = 0.92$$\n",
        "$$P(\\text{Constitution}) = 0.09, \\ P(\\text{Constitution})^{3/4} = 0.16$$\n",
        "$$P(\\text{bombastic}) = 0.01, \\ P(\\text{bombastic})^{3/4} = 0.032$$\n",
        "\n",
        "The probability of high-frequent words stayed the same (approximately), while low-frequent words now are more likely.\n",
        "\n",
        "Nice description of this algorithm can be found in [cs224n lecture notes](https://github.com/maxim5/cs224n-winter-2017/blob/master/lecture_notes/cs224n-2017-notes1.pdf).\n",
        "\n",
        "**Task** Implement Negative Sampling.\n",
        "\n",
        "Define distribution first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcX4vRBLlXy6"
      },
      "source": [
        "words_sum_count = sum(words_counter.values())\n",
        "word_distribution = np.array([(words_counter[word] / words_sum_count) ** (3 / 4) for word in index2word])\n",
        "word_distribution /= word_distribution.sum()\n",
        "\n",
        "indices = np.arange(len(word_distribution))\n",
        "\n",
        "np.random.choice(indices, p=word_distribution, size=(32, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2pzsue16Lu"
      },
      "source": [
        "class NegativeSamplingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self._embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self._out_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, inputs, targets, num_samples):\n",
        "        '''\n",
        "        inputs: (batch_size, context_size)\n",
        "        targets: (batch_size)\n",
        "        num_samples: int\n",
        "        Returns loss calculated like in the formula above\n",
        "        '''\n",
        "\n",
        "        <calc u_c (using self._embedding) & v_c (using self._out_layer)>\n",
        "\n",
        "        <obtain negative sample indices with shape (inputs.shape[0], num_samples) using np.random.choice>\n",
        "\n",
        "        <calculate v_tilde - embeddings of the negative samples (using self._out_layer)>\n",
        "\n",
        "        <calculate logsigmoid outputs (use F.logsigmoid)>\n",
        "\n",
        "        <return mean loss>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "265oljMhKdBp"
      },
      "source": [
        "Train and visualize the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wz2iRanqzlq"
      },
      "source": [
        "model = NegativeSamplingModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "negative_samples = 20\n",
        "loss_every_nsteps = 1000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for step, batch in enumerate(make_cbow_batches_iter(contexts, window_size=2, batch_size=128)):\n",
        "    <update the learning cycle accordingly>\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if step != 0 and step % loss_every_nsteps == 0:\n",
        "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps,\n",
        "                                                                    time.time() - start_time))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFik_6djvg3F"
      },
      "source": [
        "visualize_embeddings(model.embeddings.weight.data.cpu().numpy(), index2word, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4G2X-TTpzwz"
      },
      "source": [
        "### Structured Word2Vec\n",
        "\n",
        "In the paper [Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf)\n",
        "two ways of improvement of the embeddings are discussed: *Structured Skip-gram Model* and *Continuous Window Model*:   \n",
        "![](https://i.ibb.co/56w8MC8/Structured-Word2vec.png)  \n",
        "*From Two/Too Simple Adaptations of Word2Vec for Syntax Problems*\n",
        "\n",
        "In contract to the classic word2vec, each context word has its own embedding matrix. The idea is that the words order is meaningful and by learning the order embeddings learn syntax better.\n",
        "\n",
        "The disadvantage of such approach is that you have to learn much more parameters in the model.\n",
        "\n",
        "**Task** Read the paper and implement at least one of the described methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEazfh1s9eki"
      },
      "source": [
        "class CWindowModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, window_size):\n",
        "        super().__init__()\n",
        "\n",
        "        <create layers>\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        <apply 'em>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uF6iF6A9uGQ"
      },
      "source": [
        "model = CWindowModel(vocab_size=len(word2index), embedding_dim=32, window_size=2).cuda()\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "loss_every_nsteps = 1000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for step, batch in enumerate(make_cbow_batches_iter(contexts, window_size=2, batch_size=128)):\n",
        "    <copy-paste the cycle yet again>\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if step != 0 and step % loss_every_nsteps == 0:\n",
        "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps,\n",
        "                                                                    time.time() - start_time))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqDmuu7m_PB5"
      },
      "source": [
        "# Supplementary Materials\n",
        "\n",
        "## To read\n",
        "### Blogs\n",
        "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
        "[On word embeddings - Part 2: Approximating the Softmax, Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  \n",
        "[Word2Vec Tutorial - The Skip-Gram Model, Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
        "[Word2Vec Tutorial Part 2 - Negative Sampling, Chris McCormick](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
        "\n",
        "### Papers\n",
        "[Word2vec Parameter Learning Explained (2014), Xin Rong](https://arxiv.org/abs/1411.2738)  \n",
        "[Neural word embedding as implicit matrix factorization (2014), Levy, Omer, and Yoav Goldberg](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)  \n",
        "\n",
        "### Enhancing Embeddings\n",
        "[Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf)  \n",
        "[Not All Neural Embeddings are Born Equal (2014)](https://arxiv.org/pdf/1410.0718.pdf)  \n",
        "[Retrofitting Word Vectors to Semantic Lexicons (2014), M. Faruqui, et al.](https://arxiv.org/pdf/1411.4166.pdf)  \n",
        "[All-but-the-top: Simple and Effective Postprocessing for Word Representations (2017), Mu, et al.](https://arxiv.org/pdf/1702.01417.pdf)  \n",
        "\n",
        "### Sentence Embeddings\n",
        "[Skip-Thought Vectors (2015), Kiros, et al.](https://arxiv.org/pdf/1506.06726)  \n",
        "\n",
        "### Backpropagation\n",
        "[Backpropagation, Intuitions, cs231n + next parts in the Module 1](http://cs231n.github.io/optimization-2/)   \n",
        "[Calculus on Computational Graphs: Backpropagation, Christopher Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
        "\n",
        "## To watch\n",
        "[cs224n \"Lecture 2 - Word Vector Representations: word2vec\"](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)  \n",
        "[cs224n \"Lecture 5 - Backpropagation\"](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)   \n"
      ]
    }
  ]
}