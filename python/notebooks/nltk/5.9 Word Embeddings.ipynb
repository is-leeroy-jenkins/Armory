{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWzU9hqUomdU"
      },
      "source": [
        "!pip install -qq nltk==3.4\n",
        "!pip install -qq gensim==3.6.0\n",
        "!pip install -qq pandas==0.23.4\n",
        "!pip install -qq bokeh==1.0.3\n",
        "\n",
        "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
        "!unzip quora.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch6GxvZIaxMO"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXl16AOdtlDk"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErZ_TOu0vAOR"
      },
      "source": [
        "*NB. This notebook is somewhat based on the YSDA NLP course [notebook](https://github.com/yandexdataschool/nlp_course/tree/master/week01_embeddings).*\n",
        "\n",
        "Guess, you've seen such pictures already:  \n",
        "![embeddings relations](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "*From [Vector Representations of Words, Tensorflow tutorial](https://www.tensorflow.org/tutorials/representation/word2vec)*\n",
        "\n",
        "We are going to use these thingies alo-o-ot in the course.\n",
        "\n",
        "Well, we need a proper introduction, nevertheless. Do you remember how we represented sentences last time?\n",
        "\n",
        "We converted a sentence to the bag-of-words:  \n",
        "![](https://i.ibb.co/Tvw1c8S/BOW.png)\n",
        "\n",
        "And each word was represented using one-hot encoding (a vector with one at the position corresponding to the word's index and zeros at all others positions).\n",
        "\n",
        "These one-hot encoding vectors have extremely high dimensions (like, hundreds of thousands or millions). They fit their purpose - to encode information about words. But they have several disadvantages.\n",
        "\n",
        "First of all, they are almost uninterpretable. I mean, all one-hot encoding vectors are orthonormal, so you cannot say that, e.g. `man` and `men` are more similar words than `man` and `crocodiles`.\n",
        "\n",
        "But we want to. Well, NLP researchers in the past few years wanted to, cannot really speak for you.\n",
        "\n",
        "And we're gonna build vectors, that encode semantics!\n",
        "\n",
        "Look at the first picture. It shows relations encoded in the word embeddings space. Such as male-female or verb tense... whatever, just check these two links: http://bionlp-www.utu.fi/wv_demo/, https://lamyiowce.github.io/word2viz/. Go and play with this relations right now! They are funny and you'll get an insight into what the word embeddings can.\n",
        "\n",
        "There is another disadvantage of one-hot encoding vectors: their size. The word embedding vectors we are going to play with have dimensions from 50 to 600 usually. That is by a few orders of magnitude smaller than one-hot encoding vectors.\n",
        "\n",
        "This is crucial for neural networks - they can work only with sufficiently small dense vectors. Well, we'll speak about it later.\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we are going to work with [gensim](https://radimrehurek.com/gensim/) - somewhat standard word embeddings python library. We'll just superficially discuss how it works, but we'll train our model and apply a pretrained one. As a result, you're (probably) gonna understand how to work with word embeddings.\n",
        "\n",
        "In the next notebook, we'll try to work out how word embeddings work and how to implement a module to train word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkxsGQqZNjxj"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaOn69Bg1hH-"
      },
      "source": [
        "Well, nothing is interesting in mere training of the word embeddings model. We are gonna apply it to a very concrete task: [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs) from kaggle:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-X7I7nc1gyS"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "quora_data = pd.read_csv('train.csv')\n",
        "\n",
        "quora_data.sample(20)[['question1', 'question2', 'is_duplicate']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p13HdkzWKtKe"
      },
      "source": [
        "You see, the dataset consists of question pairs and you have to determine which of them are duplicates and which are not.\n",
        "\n",
        "Well, I'm not promising that we'll achieve good results right now, but still... Let's train Word2vec gensim model!\n",
        "\n",
        "*Word2vec is the most popular method of building word embeddings. We'll implement it next time, right now let's believe that it just do whatever we want.*\n",
        "\n",
        "First of all, we need to collect available texts to pass them to Word2vec model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mchv4fS_21OX"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
        "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
        "\n",
        "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
        "texts[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hZMFAmvK5b7"
      },
      "source": [
        "Next, we have to tokenize the texts. Remember, last time we used `spacy` for this purpose. Well, this time we'll use `nltk` - another great NLP library.\n",
        "\n",
        "It goes this way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTxolf8nLM-n"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokenize(texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuJceE4JLRxK"
      },
      "source": [
        "**Task** Your turn: lowercase all the texts and tokenize them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7XbnSdt4REg"
      },
      "source": [
        "tokenized_texts = [<do it>]\n",
        "\n",
        "assert all(isinstance(row, (list, tuple)) for row in tokenized_texts), \\\n",
        "    \"please convert each line into a list of tokens\"\n",
        "assert all(all(isinstance(tok, str) for tok in row) for row in tokenized_texts), \\\n",
        "    \"please convert each line into a list of tokens\"\n",
        "\n",
        "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
        "assert all(not is_latin(token) or token.islower() for tokens in tokenized_texts for token in tokens),\\\n",
        "    \"please lowercase each line\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irl7RotC5C_B"
      },
      "source": [
        "print([' '.join(row) for row in tokenized_texts[:2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kj4dC3iLdwH"
      },
      "source": [
        "And we are ready to train a small model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GNuiLio8M25"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(tokenized_texts,\n",
        "                 size=32,      # embedding vector size\n",
        "                 min_count=5,  # consider words that occured at least 5 times\n",
        "                 window=5,     # define context as a 5-word window around the target word\n",
        "                 seed=0,       # + workers=1 is to make model reproducible\n",
        "                 workers=1).wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JclToDJMNwTy"
      },
      "source": [
        "## Analyzing Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBVR2kY7LkCs"
      },
      "source": [
        "Yay, we have our own model, let's play with it!\n",
        "\n",
        "To get word's vector, well, call `get_vector`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk6Fgraj-j3c"
      },
      "source": [
        "model.get_vector('anything')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHKRy7HyLuxH"
      },
      "source": [
        "To get most similar words for the given one (guess, what):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toyNzyTB-p70"
      },
      "source": [
        "model.most_similar('bread')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A510z5gTL00E"
      },
      "source": [
        "And it can do such magic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2A_DF5E-ucq"
      },
      "source": [
        "model.most_similar(positive=['coder', 'money'], negative=['brain'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-_uKG4vNIJv"
      },
      "source": [
        "That is, who is like coder, with money and without brains.\n",
        "\n",
        "And this too:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_mzQgi4L474"
      },
      "source": [
        "model.most_similar([model.get_vector('politician') - model.get_vector('power') + model.get_vector('honesty')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5BAEyMyL2kx"
      },
      "source": [
        "Honest politician without power, isn't it just cute.\n",
        "\n",
        "**Task** Play with it. And yes, I'm serious."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FYuh4DKN1Fd"
      },
      "source": [
        "## Visualizing Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdT0eIEiN4Ja"
      },
      "source": [
        "Let's now look at the projection of the first 1000 the most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1e9yS0zBr6j"
      },
      "source": [
        "words = sorted(model.vocab.keys(),\n",
        "               key=lambda word: model.vocab[word].count,\n",
        "               reverse=True)[:1000]\n",
        "\n",
        "print(words[::100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yk5pgMXOESS"
      },
      "source": [
        "**Task** Build the matrix from these words' vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQu724f2CAh0"
      },
      "source": [
        "word_vectors = model.vectors[[model.vocab[word].index for word in words]]\n",
        "\n",
        "assert isinstance(word_vectors, np.ndarray)\n",
        "assert word_vectors.shape == (len(words), model.vectors.shape[1])\n",
        "assert np.isfinite(word_vectors).all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA3onWUbcWAo"
      },
      "source": [
        "Now we would try to project this 32 dimensional vectors to the more convenient 2D space to be able to look on them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7cgxin-OTvK"
      },
      "source": [
        "### PCA\n",
        "\n",
        "The simplest linear method of dimension reduction is __P__rincipial __C__omponent __A__nalysis.\n",
        "\n",
        "PCA builds so called principal components - set of variables along which our data has the largest variance:  \n",
        "\n",
        "![pca](https://i.stack.imgur.com/Q7HIP.gif)\n",
        "*From the great answer [https://stats.stackexchange.com/a/140579](https://stats.stackexchange.com/a/140579)*\n",
        "\n",
        "For instance, in the picture, the rotating line represents possible variants of the first principal component. If we want to project 2D set of dots to one dimension, we'll probably want to save as much information as possible. The maximum variance position of the rotating line gives us more information about the dots than all other positions.\n",
        "\n",
        "Really nice illustrations of this mechanism live [here](http://setosa.io/ev/principal-component-analysis/).\n",
        "\n",
        "To be short, project multi-dimensional space on the first two or three components and enjoy fast-and-dirty dimensional reduction.\n",
        "\n",
        "**Task** Use [sklearn PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to project data to 2D. Centre and normalize the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0fQKZw2Css4"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def get_pca_projection(word_vectors):\n",
        "    <implement me>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_9VtLl9CviW"
      },
      "source": [
        "word_vectors_pca = get_pca_projection(word_vectors)\n",
        "\n",
        "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
        "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
        "assert max(abs(1 - word_vectors_pca.std(0))) < 1e-5, \"points must have unit variance\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGyvhrk7Rlyt"
      },
      "source": [
        "Let's visualize the embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U58YxF3Cx0W"
      },
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "\n",
        "    if isinstance(color, str):\n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show:\n",
        "        pl.show(fig)\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBljy2hCC1qX"
      },
      "source": [
        "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOIU8uXnSItf"
      },
      "source": [
        "### T-SNE\n",
        "\n",
        "There is a more complicated method of data visualization. It's called t-SNE. You can gain an intuition behind it from [this](https://distill.pub/2016/misread-tsne/) article (warning: even more beautiful illustrations).\n",
        "\n",
        "**Task** Well, the same as the previous one: apply [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), normalize and center the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-nlN4_aDF9G"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    <fill me>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_YFR_rYDK2n"
      },
      "source": [
        "word_tsne = get_tsne_projection(word_vectors)\n",
        "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz6uHYMbSjuE"
      },
      "source": [
        "## Using Pretrained Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4GEypE4SokX"
      },
      "source": [
        "We can also use a pretrained embeddings model. There are a number of such models in gensim, you can call `api.info()` to get the list.\n",
        "\n",
        "Let's load a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUDRtumXXF3S"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load('glove-twitter-100')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6CST4OCyCBF"
      },
      "source": [
        "## Building Phrase Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eypMhlOSXFWN"
      },
      "source": [
        "The simplest way to obtain a phrase embedding is to average embeddings of the words in the phrase.\n",
        "\n",
        "*You are probably thinking, 'What a dumb idea, why on earth the average of embedding should contain any useful information'. Well, check [this paper](https://arxiv.org/pdf/1805.09843.pdf).*\n",
        "\n",
        "Let's do it: tokenize and lowercase the texts, calc the mean embedding for the words with known embeddings.\n",
        "\n",
        "**Task** Implement the following function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F76HGRGDEPVq"
      },
      "source": [
        "def get_phrase_embedding(model, phrase):\n",
        "    \"\"\" Calcs phrase embedding as a mean of known word embeddings in the phrase.\n",
        "    If all the words are unknown, returns zero vector.\n",
        "    :param model: KeyedVectors instance\n",
        "    :param phrase: str or list of str (tokenized text)\n",
        "    \"\"\"\n",
        "    embedding = np.zeros([model.vector_size], dtype='float32')\n",
        "\n",
        "    if isinstance(phrase, str):\n",
        "        words = word_tokenize(phrase.lower())\n",
        "    else:\n",
        "        words = phrase\n",
        "\n",
        "    <implement me>\n",
        "\n",
        "    return embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i12XsqnIXfko"
      },
      "source": [
        "vector = get_phrase_embedding(model, \"I'm very sure. This never happened to me before...\")\n",
        "\n",
        "assert np.allclose(vector[::10],\n",
        "                   np.array([ 0.30757686, -0.05861897,  0.143751  , -0.11104885, -0.96929336,\n",
        "                             -0.21928601,  0.21652265,  0.14978765,  1.4842536 ,  0.017826  ],\n",
        "                              dtype=np.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl2nBZG0WAUx"
      },
      "source": [
        "Well, we are ready to embed all the sentences in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40LezFEJFwv3"
      },
      "source": [
        "text_vectors = np.array([get_phrase_embedding(model, phrase) for phrase in tokenized_texts])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBj8XMZvWFTv"
      },
      "source": [
        "What can we do with it? Now we are able perform search of the nearest neighbours to the given phrase in our base!\n",
        "\n",
        "How are we going to define the distance?\n",
        "\n",
        "We'll use cosine similarity of two vectors:\n",
        "$$\\text{cosine_similarity}(x, y) = \\frac{x^{T} y}{||x||\\cdot ||y||}$$\n",
        "\n",
        "*It's not a [distance](https://www.encyclopediaofmath.org/index.php/Metric) strictly speaking but we still can use it to search for the vectors.*\n",
        "\n",
        "**Task** Calc the similarity between `query` embedding and `text_vectors` using `cosine_similarity` function. Find `k` vectors with highest scores and return corresponding texts from `texts` list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjw7kTQ-FP11"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_nearest(model, text_vectors, texts, query, k=10):\n",
        "    <implement me too>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2yK0twNGWaQ"
      },
      "source": [
        "results = find_nearest(model, text_vectors, texts, query=\"How do i enter the matrix?\", k=10)\n",
        "\n",
        "print('\\n'.join(results))\n",
        "\n",
        "assert len(results) == 10 and isinstance(results[0], str)\n",
        "assert results[1] == 'How do I get to the dark web?'\n",
        "assert results[4] == 'What can I do to save the world?'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AutTfxbDGhku"
      },
      "source": [
        "find_nearest(model, text_vectors, texts, query=\"How does Trump?\", k=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47Ff7heKGiGV"
      },
      "source": [
        "find_nearest(model, text_vectors, texts, query=\"Why don't i ask a question myself?\", k=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BhDEF11ZnTY"
      },
      "source": [
        "## Starting Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D_LEV8cm0z3"
      },
      "source": [
        "### Bag-of-Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xlZZKWOE2ME"
      },
      "source": [
        "Finally, we are ready to return to the classification task.\n",
        "\n",
        "We have two sentences and we are going calculate their similarity and compare it with some threshold. If the value is higher than the threshold than we'll call the sentences similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGge73gDid99"
      },
      "source": [
        "Let's start with tokenization of the questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJTjqdgiih7O"
      },
      "source": [
        "tokenized_question1 = [word_tokenize(question.lower()) for question in quora_data.question1]\n",
        "tokenized_question2 = [word_tokenize(question.lower()) for question in quora_data.question2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT0GPAFnJv-8"
      },
      "source": [
        "assert tokenized_question1[0] == ['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india', '?']\n",
        "assert tokenized_question2[2] == ['how', 'can', 'internet', 'speed', 'be', 'increased', 'by', 'hacking', 'through', 'dns', '?']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VBcipE7Ztkc"
      },
      "source": [
        "**Task** Calc the cosine similarity between the questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItJLR_ENHGtI"
      },
      "source": [
        "question1_vectors = <calc vectors for tokenized_question1>\n",
        "question2_vectors = <calc vectors for tokenized_question2>\n",
        "\n",
        "cosine_similarities = <calc similarities between the vectors in question1_vectors and question2_vectors>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5rdNs01vgoI"
      },
      "source": [
        "assert cosine_similarities.shape == (len(quora_data),), 'Check the shapes'\n",
        "\n",
        "target_similarity = cosine_similarity([get_phrase_embedding(model, tokenized_question1[1])],\n",
        "                                      [get_phrase_embedding(model, tokenized_question2[1])])[0, 0]\n",
        "assert np.allclose(cosine_similarities[1], target_similarity), 'Check your calculations'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgCVueJ4Z3DQ"
      },
      "source": [
        "Let's find the texts' similarity threshold.\n",
        "\n",
        "We are going to optimize accuracy of the similarity prediction. For instance, accuracy with threshold equal to 0 would be equal to the fraction ones in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEzgi-o5OieT"
      },
      "source": [
        "(quora_data.is_duplicate == 1).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJvse7gpO4-E"
      },
      "source": [
        "**Task** Implement the `accuracy` function that calculates accuracy with the given threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41hRIb-KSA3F"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def accuracy(cosine_similarities, threshold, labels):\n",
        "    return <implement me>\n",
        "\n",
        "thresholds = np.linspace(0, 1, 100, endpoint=False)\n",
        "plt.plot(thresholds, [accuracy(cosine_similarities, th, quora_data.is_duplicate) for th in thresholds])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOxJSE38msmX"
      },
      "source": [
        "Let's optimize over this function to find the optimal threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3LVCJwqSgZz"
      },
      "source": [
        "from scipy.optimize import minimize_scalar\n",
        "\n",
        "res = minimize_scalar(\n",
        "    lambda th: -accuracy(cosine_similarities, th, quora_data.is_duplicate), bounds=(0.5, 0.99), method='bounded'\n",
        ")\n",
        "\n",
        "best_threshold = res.x\n",
        "best_accuracy = accuracy(cosine_similarities, best_threshold, quora_data.is_duplicate)\n",
        "print('Threshold = {:.5f}, Accuracy = {:.2%}'.format(best_threshold, best_accuracy))\n",
        "\n",
        "assert best_accuracy > 0.65, 'Check yourself'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R31CunmePSLY"
      },
      "source": [
        "Well, we are a bit better than random :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8yFlaffm34A"
      },
      "source": [
        "### Tf-idf Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cpeSa-4m64H"
      },
      "source": [
        "The averaging of vectors is boring. We can use weighted average - with tf-idf weights.\n",
        "\n",
        "Let's use `TfidfVectorizer` for this task.\n",
        "\n",
        "You see, `TfidfVectorizer` returns matrix `(samples_count, words_count)`. Our embeddings is a matrix `(words_count, embedding_dim)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjwLoOHx7ChZ"
      },
      "source": [
        "model.vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VaVrx3j7I9l"
      },
      "source": [
        "The embedding of a sequence of words $w_1, \\ldots, w_k$, as we defined, it is vector $\\sum_i \\text{idf}(w_i) \\cdot \\text{embedding}(w_i)$.\n",
        "\n",
        "That means that we can multiply matrices `(samples_count, words_count) x (words_count, embedding_dim)` to obtain the embeddings for all phrases we have.\n",
        "\n",
        "But we need to have corresponding words in both matrices. That is i-th row in the first matrix correspond to the i-th column in the second matrix.\n",
        "\n",
        "To achieve it, we are going to use `vocabulary` argument of `TfidfVectorizer`.\n",
        "\n",
        "We can extract the vocabulary this way from the gensim model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCFfdtf25ukF"
      },
      "source": [
        "vocabulary = {word: vocab_element.index for word, vocab_element in model.vocab.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDo4xkQl9mND"
      },
      "source": [
        "Initialize the vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_tM8LMk5jgc"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
        "\n",
        "vectorizer.fit(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uu5gILFElhx"
      },
      "source": [
        "**Task** Apply `vectorizer` to the `quora_data` questions and obtain the phrase vectors by multiplying them on `model.vectors`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoYjVExHd_OC"
      },
      "source": [
        "tfidf_question1 = <calc it>\n",
        "tfidf_question2 = <and it>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGM1HvnwwwYj"
      },
      "source": [
        "assert tfidf_question1.shape == tfidf_question2.shape == (len(quora_data), len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Di8jXxoBxg"
      },
      "source": [
        "Check, that the text in matrices is correctly encoded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k52NtVJKf1Wl"
      },
      "source": [
        "for col in tfidf_question1[0].tocoo().col:\n",
        "    print(model.index2word[col], end=' ')\n",
        "\n",
        "print('\\n' + ' '.join(tokenized_question1[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNBcFog40fa8"
      },
      "source": [
        "Now we are able to convert the vectors matrices to vectors. That is, multiply tfidf and word2vec matrices and nomalize the result by the number of words in each sentence.\n",
        "\n",
        "**Task** Build the question vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWGN_z0UxBNL"
      },
      "source": [
        "EPS = 1e-9\n",
        "\n",
        "question1_elements_count = <calc it, add EPS to ensure you don't divide by zero>\n",
        "question2_elements_count = <and it too>\n",
        "\n",
        "assert question1_elements_count.shape == question2_elements_count.shape == (len(quora_data), 1)\n",
        "assert np.all(question1_elements_count > 0) and np.all(question2_elements_count > 0.)\n",
        "\n",
        "question1_vectors = <calc mean tfidf-weighted vectors>\n",
        "question2_vectors = <and these too>\n",
        "\n",
        "assert question1_vectors.shape == question2_vectors.shape == (len(quora_data), model.vectors.shape[1])\n",
        "\n",
        "assert np.allclose(question1_vectors[0][:10], [ 0.04672134, -0.00910798,  0.06817335,  0.00792347,  0.00907249,\n",
        "                                                0.05163505,  0.02648487, -0.05109346,  0.04752091, -0.01203835])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fMqY5tOoL9r"
      },
      "source": [
        "**Task** Evaluate the quality of these embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP46USWDy2Ah"
      },
      "source": [
        "cosine_similarities = <calc them>\n",
        "assert cosine_similarities.shape == (len(quora_data),), 'Check the shapes'\n",
        "assert np.allclose(cosine_similarities[:5], [0.99604267, 0.9558047 , 0.973884  , 0.79243606, 0.92760015])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chf2qy7uhbPF"
      },
      "source": [
        "res = minimize_scalar(\n",
        "    lambda th: -accuracy(cosine_similarities, th, quora_data.is_duplicate), bounds=(0.5, 0.99), method='bounded'\n",
        ")\n",
        "\n",
        "best_threshold = res.x\n",
        "best_accuracy = accuracy(cosine_similarities, best_threshold, quora_data.is_duplicate)\n",
        "print('Threshold = {:.5f}, Accuracy = {:.2%}'.format(best_threshold, best_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvqrFUS6vVhh"
      },
      "source": [
        "## Implementing Word-level Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CXcr-ypzGXg"
      },
      "source": [
        "!wget -O ukr_rus.train.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vAK0SWXUqei4zTimMvIhH3ufGPsbnC_O\"\n",
        "!wget -O ukr_rus.test.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1W9R2F8OeKHXruo2sicZ6FgBJUTJc8Us_\"\n",
        "!wget -O fairy_tale.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1sq8zSroFeg_afw-60OmY8RATdu_T1tej\"\n",
        "\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1d7OXuil646jUeDS1JNhP9XWlZogv6rbu'})\n",
        "downloaded.GetContentFile('cc.ru.300.vec.zip')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1yAqwqgUHtMSfGS99WLGe5unSCyIXfIxi'})\n",
        "downloaded.GetContentFile('cc.uk.300.vec.zip')\n",
        "\n",
        "!unzip cc.ru.300.vec.zip\n",
        "!unzip cc.uk.300.vec.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RqUeOXxws8y"
      },
      "source": [
        "Let's implement a simple machine translator.\n",
        "\n",
        "The idea is based on the paper [Word Translation Without Parallel Data](https://arxiv.org/pdf/1710.04087.pdf). There are lots of interesting things in the repo: [https://github.com/facebookresearch/MUSE](https://github.com/facebookresearch/MUSE).\n",
        "\n",
        "And we are going to translate from Ukrainian to Russian. They are quite similar languages with similar syntax. This is why we can substitute words from one language with words from another and expect something coherent in the result.\n",
        "\n",
        "That is, we are going to learn how embeddings from one language correspond to embeddings from another, like this:\n",
        "\n",
        "![](https://raw.githubusercontent.com/facebookresearch/MUSE/master/outline_all.png)\n",
        "\n",
        "Than we will simply map the source word (the word in the sentence we want to translate) to the target embedding space and take the word with the nearest embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjPj9FTRry0U"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")\n",
        "uk_emb = KeyedVectors.load_word2vec_format(\"cc.uk.300.vec\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rGx4TXWFJ65"
      },
      "source": [
        "Look at the pair `серпень-август` (which are translation, means august)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkHer36xyh4n"
      },
      "source": [
        "ru_emb.most_similar([ru_emb[\"август\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RSDixWvylEP"
      },
      "source": [
        "uk_emb.most_similar([uk_emb[\"серпень\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwmm3YQ1yl1U"
      },
      "source": [
        "ru_emb.most_similar([uk_emb[\"серпень\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAsW7oxszE_I"
      },
      "source": [
        "def load_word_pairs(filename):\n",
        "    uk_ru_pairs = []\n",
        "    uk_vectors = []\n",
        "    ru_vectors = []\n",
        "    with open(filename, \"r\", encoding='utf8') as inpf:\n",
        "        for line in inpf:\n",
        "            uk, ru = line.rstrip().split(\"\\t\")\n",
        "            if uk not in uk_emb or ru not in ru_emb:\n",
        "                continue\n",
        "            uk_ru_pairs.append((uk, ru))\n",
        "            uk_vectors.append(uk_emb[uk])\n",
        "            ru_vectors.append(ru_emb[ru])\n",
        "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)\n",
        "\n",
        "\n",
        "uk_ru_train, X_train, Y_train = load_word_pairs(\"ukr_rus.train.txt\")\n",
        "uk_ru_test, X_test, Y_test = load_word_pairs(\"ukr_rus.test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z6ts7DC0XmN"
      },
      "source": [
        "### Learning the mapping from the embedding spaces\n",
        "\n",
        "We have pairs of corresponding words. So we have to find a mapping which would map their embeddings to be as near as possible.\n",
        "\n",
        "$$W^*= \\arg\\min_W ||WX - Y||_F, \\text{where} ||*||_F - \\text{Frobenius norm}$$\n",
        "\n",
        "This function is similar to the linear regression (without bias).\n",
        "\n",
        "**Task** Implement it - use `LinearRegression` from sklearn with `fit_intercept=False`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fraTOQtu1YWI"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "mapping = LinearRegression(fit_intercept=False).fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrzRk3ja1b_6"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Quax6HnF1aON"
      },
      "source": [
        "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
        "ru_emb.most_similar(august)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih1GLNZt1nZX"
      },
      "source": [
        "Expected that the top contains different months, but `август` is not the first.\n",
        "\n",
        "We are going to evaluate the mapping by precision@k metric with k = 1, 5, 10.\n",
        "\n",
        "**Task** Implement following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnmrLp9y2gNI"
      },
      "source": [
        "def precision(pairs, mapped_vectors, topn=1):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
        "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
        "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
        "    :returns:\n",
        "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
        "    \"\"\"\n",
        "    assert len(pairs) == len(mapped_vectors)\n",
        "    <implement it>\n",
        "    return precision_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1NIvhSH2olG"
      },
      "source": [
        "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
        "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
        "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ml_w1Tl2r7Y"
      },
      "source": [
        "assert precision(uk_ru_test, X_test) == 0.0\n",
        "assert precision(uk_ru_test, Y_test) == 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d9KQHMr2tx8"
      },
      "source": [
        "precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n",
        "precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)\n",
        "\n",
        "assert precision_top1 >= 0.635\n",
        "assert precision_top5 >= 0.813"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNbDTP502urT"
      },
      "source": [
        "### Improving Mapping\n",
        "\n",
        "It can be proven that the mapping with orthogonal constraint is better:\n",
        "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, where: } W^TW = I$$\n",
        "\n",
        "You can find it using SVD:\n",
        "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
        "\n",
        "$$W^*=UV^T$$\n",
        "\n",
        "**Task** Implement the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9de8XZ_F3v53"
      },
      "source": [
        "def learn_transform(X_train, Y_train):\n",
        "    \"\"\"\n",
        "    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n",
        "    \"\"\"\n",
        "    <calculate it>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WeCadzN382y"
      },
      "source": [
        "W = learn_transform(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6qaMb0E3-f9"
      },
      "source": [
        "ru_emb.most_similar([np.matmul(uk_emb[\"серпень\"], W)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nn58crh4AH0"
      },
      "source": [
        "assert precision(uk_ru_test, np.matmul(X_test, W)) >= 0.653\n",
        "assert precision(uk_ru_test, np.matmul(X_test, W), 5) >= 0.824"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqgcYk-c4DE5"
      },
      "source": [
        "### Writing the translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwi70fP6FaAN"
      },
      "source": [
        "Now we are ready to implement the translation function. It should find the nearest vector in the target (Russian) embedding space and return the source word if it is not in the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0etAHUks4JOr"
      },
      "source": [
        "with open(\"fairy_tale.txt\", \"r\") as in f:\n",
        "    uk_sentences = [line.rstrip().lower() for line in in f]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK_FJGmn4N7V"
      },
      "source": [
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        sentence - sentence in Ukrainian (str)\n",
        "    :returns:\n",
        "        translation - sentence in Russian (str)\n",
        "\n",
        "    * find ukrainian embedding for each word in sentence\n",
        "    * transform ukrainian embedding vector\n",
        "    * find nearest russian word and replace\n",
        "    \"\"\"\n",
        "    <implement it>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H47pbFyk4P6D"
      },
      "source": [
        "assert translate(\".\") == \".\"\n",
        "assert translate(\"1 , 3\") == \"1 , 3\"\n",
        "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAVWK7mE4RYU"
      },
      "source": [
        "for sentence in uk_sentences:\n",
        "    print(\"src: {}\\ndst: {}\\n\".format(sentence, translate(sentence)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5GrChTeFqIg"
      },
      "source": [
        "# Supplementary Materials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwffxpbmFwDh"
      },
      "source": [
        "## To read\n",
        "### Basic knowledge:  \n",
        "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
        "[Deep Learning, NLP, and Representations, Christopher Olah](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)  \n",
        "\n",
        "### How to clusterize embeddings:  \n",
        "[Making Sense of Word Embeddings (2016), Pelevina et al](http://anthology.aclweb.org/W16-1620)    \n",
        "\n",
        "### How to evaluate embeddings:\n",
        "[Evaluation methods for unsupervised word embeddings (2015), T. Schnabel](http://www.aclweb.org/anthology/D15-1036)  \n",
        "[Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance (2016), B. Chiu](https://www.aclweb.org/anthology/W/W16/W16-2501.pdf)  \n",
        "[Problems With Evaluation of Word Embeddings Using Word Similarity Tasks (2016), M. Faruqui](https://arxiv.org/pdf/1605.02276.pdf)  \n",
        "[Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure (2016), Oded Avraham, Yoav Goldberg](https://arxiv.org/pdf/1611.03641.pdf)  \n",
        "[Evaluating Word Embeddings Using a Representative Suite of Practical Tasks (2016), N. Nayak](https://cs.stanford.edu/~angeli/papers/2016-acl-veceval.pdf)  \n",
        "\n",
        "\n",
        "## To watch\n",
        "[Word Vector Representations: word2vec, Lecture 2, cs224n](https://www.youtube.com/watch?v=ERibwqs9p38)"
      ]
    }
  ]
}