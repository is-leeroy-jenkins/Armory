{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Probability: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Trial or experiment`: An action that results in a certain outcome with a certain likelihood\n",
    "\n",
    "- `Sample space`: This encompasses all potential outcomes of a given experiment\n",
    "\n",
    "- `Event`: This denotes a non-empty portion of the sample space\n",
    "\n",
    "- Probability is a measure of the likelihood of an event occurring when an experiment is conducted.\n",
    "\n",
    "- The probability of event A with one outcome is equal to the chance of event A divided by the chance of all possible events. \n",
    "\n",
    "- The value of probability ranges from 0 to 1, with the sample space embodying the complete set of potential outcomes, denoted as P(S) = 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Statistical Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two events are defined as independent if the occurrence of one event doesnâ€™t influence the likelihood of the other eventâ€™s occurrence. \n",
    "\n",
    "- Events `A` and `B` are independent precisely when `P(A and B) = P(A)P(B)`, where `P(A)` and `P(B)` are the respective probabilities of events `A` and `B` happening.\n",
    "\n",
    "- `Complementary event`: The complementary event to `A`, signified as `Aâ€™`, encompasses the probability of all potential outcomes in the sample space not included in `A`. \n",
    "\n",
    "- `Union and intersection`: The complementary event to `A`, signified as `Aâ€™`, encompasses the probability of all potential outcomes in the sample space not included in `A`. \n",
    "\n",
    "- `Mutually exclusive`: When two events have no shared outcomes, they are viewed as mutually exclusive. In other words, if `A` and `B` are mutually exclusive events, then `P(A âˆ© B) = 0`. \n",
    "\n",
    "- `Independent`: Two events are deemed independent when the occurrence of one doesnâ€™t impact the occurrence of the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  Discrete Random Variable\n",
    "\n",
    "- A discrete random variable refers to a variable that can assume a finite or countably infinite number \n",
    "of potential outcomes. \n",
    "\n",
    "- The probability distribution of a discrete random variable assigns a certain likelihood to each potential \n",
    "outcome the variable could adopt.\n",
    "\n",
    "- `Probability Mass Function (PMF)` - correlates each possible outcome of the variable to its likelihood of occurrence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PMF is bound by two principles:\n",
    "\n",
    "\n",
    " - It must be non-negative across all potential values of the random variable\n",
    "\n",
    " - The total sum of probabilities for all possible outcomes should equate to 1\n",
    "\n",
    " - The expected value of a discrete random variable offers an insight into its central tendency, computed as the probability-weighted average of its possible outcomes. \n",
    " \n",
    " - This expected value is signified as `E[X]`, with `X` representing the random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Probability Density Function` (PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A tool used to describe the distribution of a continuous random variable. \n",
    "\n",
    "- It can be used to calculate the probability of a value falling within a specific range. \n",
    "\n",
    "- It helps determine the chances of a continuous variable, `X`, having a value within the interval `[a, b]`, or in statistical terms,\n",
    " `P  ( A < X < B )` \n",
    "\n",
    "- For continuous variables, the probability of a single value occurring is always 0, which is in contrast to \n",
    "discrete variables that can assign non-0 probabilities to distinct values. \n",
    "\n",
    "- PDFs provide a way to estimate the likelihood of a value falling within a given range instead of a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Maximum-Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maximum likelihood is a statistical approach, that is used to estimate the parameters of a probability distribution. \n",
    "\n",
    "- The objective is to identify the parameter values that maximize the likelihood of observing the data, essentially determining the parameters most likely to have generated the data.\n",
    "\n",
    "- Suppose we have a random sample,  `X =  { X 1, â€¦ , X n}` , from a population with a probability distribution  `f( x | ðœ½ )`, where `Î¸` is a vector of parameters. \n",
    "\n",
    "- The likelihood of observing the sample, X, given the parameters, Î¸, is defined as the product of the individual probabilities of observing each data point:\n",
    "\n",
    "```\n",
    "\n",
    "        L( ðœ½  | X ) =  f ( X  | ðœ½ )\n",
    "\n",
    "```\n",
    "\n",
    "- In case of having independent and identically distributed observations, the likelihood function can be expressed as the product of the univariate density functions, each evaluated at the corresponding observation:\n",
    "\n",
    "```\n",
    "\n",
    "        L( ðœ½ | X) = f( X1| ðœ½ ) f( X2 | ðœ½ ) â€¦f( Xn| ðœ½ ) \n",
    "\n",
    "```\n",
    "\n",
    "- The `maximum likelihood estimate (MLE)` is the parameter vector value that offers the maximum value for the likelihood function across the parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In many cases, itâ€™s more convenient to employ the natural logarithm of the likelihood function, referred to as the `log-likelihood`. \n",
    "\n",
    "- The peak of the `log-likelihood` happens at the identical parameter vector value as the likelihood functionâ€™s maximum, and the conditions required for a maximum (or minimum) are acquired by equating the `log-likelihood` derivatives with respect to each parameter to 0. \n",
    "\n",
    "\n",
    "- If the `log-likelihood` is differentiable with respect to the parameters, these conditions result in a set of \n",
    "equations that can be solved numerically to derive the `MLE`. \n",
    "\n",
    "- `MLE` is often used to estimate the coefficients that define the relationship between input features and the target variable. \n",
    "\n",
    "- `MLE` helps find the values for the coefficients that maximize the likelihood of observing the given data under the assumed linear regression model, improving the accuracy of the predictions.\n",
    "\n",
    "- The `MLE`s of the parameters, `Î¸`, are the values that maximize the likelihood function. \n",
    "\n",
    "- In other words, the `MLE`s are the values of Î¸ that make the observed data, X, most probable.\n",
    "\n",
    "- To find the `MLE`s, we typically take the natural logarithm of the likelihood function, as it is often easier to work with the logarithm of a product than with the product itself:\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "            lnL( ðœ½ | X ) = lnf( X 1| ðœ½) + lnf( X 2| ðœ½ ) + â€¦ + lnf( X n| ðœ½ )\n",
    "\n",
    "```\n",
    "\n",
    "-  The `MLE`s are determined by equating the partial derivatives of the `log-likelihood` function with respect to each parameter to 0 and then solving these equations for the parameters\n",
    "\n",
    "- Once the `MLE`s have been found, they can be used to make predictions about the population based on the sample data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bayesian Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A statistical approach that involves updating our beliefs or probabilities about a quantity of interest based on new data. \n",
    "\n",
    "- In Bayesian estimation, we start with prior beliefs about the quantity of interest, which are expressed as a probability distribution. \n",
    "\n",
    "- These prior beliefs are updated as we collect new data. \n",
    "\n",
    "- The updated beliefs are represented as a posterior distribution. \n",
    "\n",
    "- The Bayesian framework provides a systematic way of updating prior beliefs with new data.\n",
    "\n",
    "- The choice of prior distribution is important, as it reflects our beliefs about the quantity of interest before collecting any data. \n",
    "\n",
    "- The prior distribution can be chosen based on prior knowledge or previous studies. \n",
    "\n",
    "- If no prior knowledge is available, a non-informative prior can be used, such as a uniform distribution.\n",
    "\n",
    "- Once the posterior distribution is calculated, it can be used to make predictions about the quantity of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In real-world business environments, one important task is to define the dataset from all possible sources of data, explore the gathered data to find the best method for preprocessing it, and ultimately decide on the ML and natural language models that fit the problem \n",
    "and the underlying data best. \n",
    "\n",
    "- Data exploration is an iterative process that involves visualizing and analyzing the data, looking for patterns and relationships, and identifying potential issues or outliers. \n",
    "\n",
    "- Data exploration is an important, initial step in the ML workflow that involves analyzing and understanding the data before building a ML model. \n",
    "\n",
    "- The goal of data exploration is to gain insights about the data, identify patterns, detect anomalies, and prepare the data for modeling. \n",
    "\n",
    "\n",
    "- In `Natural Language Processing (NLP)`, the data can be quite complex, as it often includes text and speech data that can be unstructured and difficult to analyze. \n",
    "\n",
    "- The first step of any NLP or ML solution starts with exploring the data to learn more about it, which helps us decide on our path to tackle the problem.\n",
    "\n",
    "- Preprocessing methods such as `tokenization`, `stemming`, and `lemmatization` can be employed. \n",
    "\n",
    "- It is important to note that employing effective preprocessing techniques can significantly enhance the performance and accuracy of ML models, making them more robust and reliable.\n",
    "\n",
    "- Once the data has been preprocessed and explored, we can start building our ML models. \n",
    "\n",
    "- There is no single magical solution that works for all ML problems, so itâ€™s important to carefully consider which models are best suited for the data and the problem at hand.\n",
    "\n",
    "\n",
    "- Data exploration helps in choosing the right ML algorithm and determining the best set of features to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### `Data visualization`: \n",
    "\n",
    "- Data visualization involves depicting data through graphical or pictorial formats. \n",
    "\n",
    "- It enables visual exploration of data, providing insights into its distribution, patterns, and relationships. \n",
    "\n",
    "- Widely employed techniques in data visualization encompass scatter plots, bar charts, heatmaps, box plots, and correlation matrices.\n",
    "\n",
    "#### `Data cleaning`: \n",
    "\n",
    "- Data cleaning is a step of preprocessing where we identify the errors, inconsistencies, and missing values and correct them. \n",
    "\n",
    "- It affects the final results of the model since ML models are sensitive to errors in the data. \n",
    "\n",
    "- Removing duplicates and filling in missing values are some of the common data cleaning techniques.\n",
    "\n",
    "#### `Feature engineering`: \n",
    "\n",
    "- Feature engineering plays a crucial role in optimizing the effectiveness of machine learning models by crafting new features from existing data. \n",
    "\n",
    "- This process involves not only identifying pertinent features but also transforming the existing ones and introducing novel features. \n",
    "\n",
    "- Various feature engineering techniques, including scaling, normalization, dimensionality reduction, and feature selection, contribute to refining the overall performance of the models.\n",
    "\n",
    "####  `Statistical analysis`: \n",
    "\n",
    "- Statistical analysis utilizes a range of statistical techniques to scrutinize data, revealing valuable insights into its inherent properties. \n",
    "\n",
    "- Essential statistical methods include hypothesis testing, regression analysis, and time series analysis, all of which contribute to a comprehensive understanding of the dataâ€™s characteristics.\n",
    "\n",
    "####  `Domain knowledge`: \n",
    "\n",
    "- Leveraging domain knowledge entails applying a pre-existing understanding of the data domain to extract insights and make informed decisions. \n",
    "\n",
    "- This knowledge proves valuable in recognizing pertinent features, interpreting results, and choosing the most suitable ML algorithm for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Involves recognizing and rectifying 0r eliminating errors, inconsistencies, and inaccuracies within a dataset. \n",
    "\n",
    "- A crucial phase in data preparation for ML significantly influences the accuracy and performance of a model, relying on the \n",
    "quality of the data used for training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dropping rows`: \n",
    "\n",
    "- Addressing missing data can involve a straightforward approach of discarding rows that contain such values. \n",
    "\n",
    "- Exercising caution is paramount when employing this method as excessive row removal may result in the loss of valuable data, impacting the \n",
    "overall accuracy of the model. \n",
    "\n",
    "- We usually use this method when we have a few rows in our dataset, and we have a few rows with missing values. In this case, removing a few rows can \n",
    "\n",
    "`Dropping columns`: \n",
    "\n",
    "- Another approach is to drop the columns that contain missing values. \n",
    "\n",
    "- This method can be effective if the missing values are concentrated in a few columns and if those columns are not important for the analysis. \n",
    "\n",
    "- It is better to perform some sort of correlation analysis to see the correlation of the values in these columns with the target class or value before dropping these columns.\n",
    "\n",
    "`Mean/Median/Mode Imputation`: \n",
    "\n",
    "- Mean, median, and mode imputation entail substituting missing values with the mean, median, or mode derived from the non-missing values within the corresponding column. \n",
    "\n",
    "- This method is easy to implement and can be effective when the missing values are few and randomly distributed. \n",
    "\n",
    "- However, it can also introduce bias and affect the variability of the data.\n",
    "\n",
    "\n",
    "`Regression Imputation`: \n",
    "\n",
    "- Regression imputation involves predicting the missing values based on the values of other variables in the dataset. \n",
    "\n",
    "- This method can be effective when the missing values are related to other variables in the dataset, but it requires a regression model to be built for each column with missing values.\n",
    "\n",
    "\n",
    "`Multiple Imputation`: \n",
    "\n",
    "- Multiple imputation encompasses generating multiple imputed datasets through statistical models, followed by amalgamating the outcomes to produce a conclusive dataset. \n",
    "\n",
    "- This approach proves efficacious, particularly when dealing with non-randomly distributed missing values and a substantial number of gaps in the dataset.\n",
    "\n",
    "\n",
    "`K-Nearest Neighbor Imputation`: \n",
    "\n",
    "- K-nearest neighbor imputation entails identifying the k-nearest data points to the missing value and utilizing their values to impute the absent value. \n",
    "\n",
    "- This method can be effective when the missing values are clustered together in the dataset. \n",
    "\n",
    "- In this approach, we can find the most similar records to the dataset to the record that has the missing value, and then use the mean of the values of those records for that specific record as the missed value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eliminating duplicates is a prevalent preprocessing measure thatâ€™s employed to cleanse datasets by detecting and removing identical records. \n",
    "\n",
    "- The occurrence of duplicate records may be attributed to factors such as data entry errors, system glitches, or data merging processes. \n",
    "\n",
    "- The presence of duplicates can skew models and yield inaccurate insights. \n",
    "\n",
    "- If two or more rows have the same values in all the columns, they are considered duplicates. \n",
    "\n",
    "- In some cases, it may be necessary to compare only a subset of columns if certain columns are more prone to duplicates.\n",
    "\n",
    "- Another method is to use a unique identifier column to identify duplicates\n",
    "\n",
    "- After identifying the duplicate records, the next step is to decide which records to keep and which ones to remove. \n",
    "\n",
    "- One approach is to keep the first occurrence of a duplicate record and remove all subsequent occurrences. \n",
    "\n",
    "- Another approach is to keep the record with the most complete information, or the record with the most recent timestamp.\n",
    "\n",
    "- Itâ€™s crucial to recognize that the removal of duplicates might lead to a reduction in dataset size, potentially affecting the performance of ML models. \n",
    "\n",
    "- Assessing the impact of duplicate removal on both the dataset and the ML model is essential. \n",
    "\n",
    "- In some cases, it may be necessary to keep duplicate records if they contain important information that cannot be obtained from other records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Standardizing and Transforming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This process involves scaling and normalizing the numerical features of the dataset to make them easier to interpret and compare. \n",
    "\n",
    "- The main objective of standardizing and transforming data is to enhance the accuracy and performance of a ML model by mitigating the influence of features with diverse scales and ranges. \n",
    "\n",
    "- A widely used method for standardizing data is referred to as `standardization` or `Z-score normalization`. This technique involves transforming each feature such that it has a mean of zero and a standard deviation of one. \n",
    "\n",
    "```\n",
    "\n",
    "        xâ€² =  ( x âˆ’ mean ( x ) ) / std ( x ) \n",
    "\n",
    "```\n",
    "\n",
    "- Here, `x` represents the feature, `mean(x)` denotes the mean of the feature, `std(x)` indicates the standard deviation of the feature, and `xâ€™` represents the new value assigned to the feature. \n",
    "\n",
    "- The range of each feature is adjusted to be centered around zero, which makes it easier to compare features and prevents features with large values from dominating the analysis.\n",
    "\n",
    "\n",
    "- Another technique for transforming data is `Min-Max Scaling`. This method rescales the data to a consistent range of values, commonly ranging between 0 and 1. The formula for min-max scaling is shown here:\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "        xâ€² =  ( x âˆ’ min ( x ) ) /  ( max ( x ) âˆ’ min ( x ) ) \n",
    "        \n",
    "\n",
    "```\n",
    "\n",
    "- In this equation, `x` represents the feature, `min(x)` signifies the minimum value of the feature, and `max(x)` denotes the maximum value of the feature. \n",
    "\n",
    "- Min-max scaling proves beneficial when the precise distribution of the data is not crucial, but there is a need to standardize the data for meaningful comparisons across different features.\n",
    "\n",
    "- Transforming data can also involve changing the distribution of the data. \n",
    "\n",
    "- A frequently applied transformation is the `Log Transformation`, which is employed to alleviate the influence of outliers and skewness within the data. This transformation involves taking the logarithm of the feature values which can help to normalize the distribution and reduce the influence of extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Outliers are data points that markedly deviate from the rest of the observations in a dataset. \n",
    "\n",
    "- Their occurrence may stem from factors such as measurement errors, data corruption, or authentic extreme values. \n",
    "\n",
    "- The presence of outliers can wield a substantial influence on the outcomes of ML models, introducing distortion to the data and disrupting the relationships between variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Removing Outliers`: \n",
    "\n",
    "- One straightforward approach involves eliminating observations identified as outliers from the dataset. \n",
    "\n",
    "- Exercising caution is paramount when adopting this method as excessive removal of observations may result in the loss of valuable information and potentially introduce bias to the analysis results.\n",
    "\n",
    "\n",
    "#### `Transforming Data`: \n",
    "\n",
    "- Applying mathematical functions such as logarithms or square roots to transform the data can mitigate the influence of outliers. \n",
    "\n",
    "- For instance, taking the logarithm of a variable can alleviate the impact of extreme values, given the slower rate of increase in the \n",
    "logarithmic scale compared to the original values.\n",
    "\n",
    "\n",
    "#### `Winsorizing`:\n",
    "\n",
    "- Winsorizing is a technique that entails substituting extreme values with the nearest highest or lowest value in the dataset. \n",
    "\n",
    "- Employing this method aids in maintaining the sample size and overall distribution of the data.\n",
    "\n",
    "#### `Imputing Values`: \n",
    "\n",
    "- Imputation involves replacing missing or extreme values with estimated values derived from the remaining observations in the dataset. \n",
    "\n",
    "- For instance, substituting extreme values with the median or mean of the remaining observations is a common imputation technique.\n",
    "\n",
    "#### `Robust Statistical Methods`: \n",
    "\n",
    "- Robust statistical methods exhibit lower sensitivity to outliers, leading to more accurate results even in the presence of such extreme values. \n",
    "\n",
    "- For instance, opting for the median instead of the mean can effectively diminish the influence of outliers on the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Error Correction"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
