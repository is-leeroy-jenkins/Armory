{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pillars of Data Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- Data analysis is not a modern invention born of computers; it is the contemporary expression of a long\n",
    "mathematical and scientific tradition. From early census tabulations and astronomical observations to\n",
    "modern machine learning systems, the same intellectual questions recur: How is data structured? What\n",
    "can be inferred from limited observations? How certain are our conclusions? Technology has changed\n",
    "the *scale* of analysis, but not its *essence*.\n",
    "\n",
    "- The **Pillars of Data Analysis** describe the enduring mathematical and logical foundations that make\n",
    "analysis coherent, interpretable, and trustworthy. Each pillar represents a lineage of ideas refined over\n",
    "time and adapted to new contexts. Together, they form an integrated framework that supports descriptive\n",
    "analytics, statistical inference, predictive modeling, and decision-making under uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar I: Set Theory and Mathematical Structure\n",
    "\n",
    "- Set theory provides the most fundamental abstraction in mathematics: the idea that objects can be\n",
    "grouped into well-defined collections. In data analysis, this abstraction is unavoidable. A dataset is\n",
    "nothing more than a set of observations, and each variable defines a set of allowable values. Early\n",
    "statistical work relied implicitly on set concepts long before Cantor formalized them.\n",
    "\n",
    "- Historically, set theory enabled mathematicians to rigorously define relations, equivalence classes, and\n",
    "hierarchies. These same ideas underpin modern data systems, particularly relational databases and\n",
    "schema design. Operations such as filtering rows, identifying duplicates, or defining cohorts are\n",
    "applications of subset selection and set membership. Even data quality rulesâ€”such as uniqueness or\n",
    "referential integrityâ€”are set-theoretic constraints.\n",
    "\n",
    "- In modern analytics, set theory supports scalable querying, joins, and segmentation across massive\n",
    "datasets. Distributed systems still operate on set-based abstractions even when execution is parallelized.\n",
    "Without a clear understanding of set structure, analysts risk ambiguous definitions and inconsistent\n",
    "results. This pillar ensures precision in defining *what data are included* and *why*.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar II: Functions and Mappings\n",
    "\n",
    "- Functions formalize the idea that one quantity depends on another. Historically, this concept matured\n",
    "during the development of calculus and mathematical analysis, providing a language for describing\n",
    "change and dependency. In data analysis, functions describe how raw data are transformed into derived\n",
    "features, metrics, and predictions. Every analytical step implicitly defines a mapping from inputs to\n",
    "outputs.\n",
    "\n",
    "- Viewing analysis through a functional lens clarifies assumptions and boundaries. It forces analysts to\n",
    "consider domains, codomains, and whether transformations preserve or discard information. Aggregations,\n",
    "for example, are many-to-one functions that intentionally lose detail in exchange for summary. Feature\n",
    "engineering pipelines are compositions of functions, each with its own effect on interpretability.\n",
    "\n",
    "- Modern analytics systems emphasize functional composition through pipelines, workflows, and modular\n",
    "design. This approach improves reproducibility and testing while aligning mathematical reasoning with\n",
    "software architecture. Functions also provide a bridge between theory and implementation, ensuring that\n",
    "analytical intent is preserved in code. This pillar reinforces disciplined thinking about transformations\n",
    "and dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar III: Linear Algebra\n",
    "\n",
    "- Linear algebra arose from the need to solve systems of equations efficiently, but it has become central to\n",
    "nearly all modern data analysis. When data expanded beyond a handful of variables, matrix\n",
    "representations offered a compact and powerful way to reason about relationships. Observations become\n",
    "vectors, datasets become matrices, and models become transformations in vector spaces.\n",
    "\n",
    "- Historically, multivariate statistics and econometrics relied heavily on linear algebra to model complex\n",
    "systems. With the advent of machine learning, its importance only increased. Techniques such as\n",
    "regression, principal component analysis, and factor models are fundamentally linear algebraic. Even\n",
    "nonlinear models often rely on linear approximations at their core.\n",
    "\n",
    "- Modern applications leverage highly optimized linear algebra routines on specialized hardware. Despite\n",
    "this computational sophistication, the underlying ideas remain unchanged: projection, rotation, span,\n",
    "and rank. Analysts who understand these concepts can diagnose multicollinearity, interpret latent\n",
    "dimensions, and recognize when models are ill-posed. Linear algebra provides the structural backbone\n",
    "for high-dimensional reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar IV: Probability Theory\n",
    "\n",
    "- Probability theory emerged from attempts to quantify uncertainty in games of chance, but its broader\n",
    "impact lies in modeling randomness in the natural and social worlds. In data analysis, probability\n",
    "reframes observations as realizations of underlying stochastic processes. This perspective acknowledges\n",
    "that data are noisy, incomplete, and influenced by chance.\n",
    "\n",
    "- Historically, probability enabled the formal treatment of expectation, variance, and distributional\n",
    "assumptions. These ideas underpin statistical modeling and risk assessment. In modern analytics,\n",
    "probability supports predictive distributions rather than single-point estimates. This shift is essential\n",
    "for responsible decision-making.\n",
    "\n",
    "- Advances in computation have made probabilistic models feasible at unprecedented scales. Bayesian\n",
    "approaches, once limited by manual calculation, are now widely applied. Probability theory ensures that\n",
    "uncertainty is quantified rather than ignored. This pillar guards against overconfidence and false\n",
    "precision in analytical conclusions.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar V: Statistics and Inference\n",
    "\n",
    "- Statistics developed as societies sought to understand populations through partial observation. Census\n",
    "data, agricultural experiments, and public health studies all required methods to generalize from samples.\n",
    "The discipline formalized tools for estimation, hypothesis testing, and uncertainty quantification. These\n",
    "methods remain central to data analysis today.\n",
    "\n",
    "- Statistical inference addresses a fundamental tension: data are finite, but conclusions often concern\n",
    "broader populations. Techniques such as confidence intervals and hypothesis tests explicitly acknowledge\n",
    "this limitation. Historically, these methods provided guardrails against spurious findings and random\n",
    "variation masquerading as signal.\n",
    "\n",
    "- In modern analytics, statistical thinking remains essential even as machine learning grows in prominence.\n",
    "Model validation, cross-validation, and experimental design all rely on statistical principles. Advances in\n",
    "robust and nonparametric methods have expanded applicability. This pillar ensures that conclusions are\n",
    "grounded in evidence rather than coincidence.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar VI: Optimization and Calculus\n",
    "\n",
    "- Calculus and optimization formalize the study of change and extremal behavior. In data analysis, most\n",
    "models are defined implicitly as solutions to optimization problems. Rather than specifying outcomes\n",
    "directly, analysts define objectives and allow algorithms to search for optimal parameters. This approach\n",
    "connects mathematical theory with empirical modeling.\n",
    "\n",
    "- Historically, optimization techniques evolved from physical and economic problems. These same methods\n",
    "now drive regression, classification, and forecasting models. Gradient-based methods translate calculus\n",
    "into practical algorithms. Constraints and regularization incorporate domain knowledge and prevent\n",
    "overfitting.\n",
    "\n",
    "- Modern machine learning relies on large-scale optimization, often under severe computational constraints.\n",
    "Understanding convergence, stability, and local minima is critical for interpreting results. This pillar\n",
    "explains why models succeed, fail, or behave unpredictably. Optimization theory provides insight into\n",
    "both performance and limitations.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar VII: Information Theory\n",
    "\n",
    "- Information theory reframed uncertainty as a measurable quantity. Developed to optimize communication\n",
    "systems, it introduced concepts such as entropy and information content. These ideas translate naturally\n",
    "to data analysis, where the goal is to extract meaningful signal from noise. Information theory provides a\n",
    "quantitative basis for this extraction.\n",
    "\n",
    "- Historically, these concepts clarified limits on compression and transmission. In analytics, they explain\n",
    "why some variables are more informative than others. Measures such as information gain guide feature\n",
    "selection and model structure. Cross-entropy losses connect probabilistic modeling with optimization.\n",
    "\n",
    "- Modern applications include natural language processing, decision trees, and representation learning.\n",
    "Information-theoretic metrics also support model comparison and diagnostics. This pillar highlights the\n",
    "relationship between uncertainty, prediction, and learning. It reinforces the idea that not all data\n",
    "contribute equally to insight.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar VIII: Discrete Mathematics and Algorithms\n",
    "\n",
    "- Discrete mathematics focuses on finite structures and logical processes. Its development paralleled the\n",
    "rise of computer science, providing tools for reasoning about computation. In data analysis, discrete\n",
    "structures appear in graphs, categorical variables, and rule-based systems. Algorithmic efficiency is as\n",
    "important as mathematical correctness.\n",
    "\n",
    "- Historically, combinatorics and graph theory addressed problems of arrangement and connectivity.\n",
    "These ideas now underpin network analysis, recommendation systems, and clustering algorithms.\n",
    "Understanding algorithmic complexity helps analysts anticipate scalability issues. It also informs design\n",
    "choices when processing large datasets.\n",
    "\n",
    "- Modern analytics increasingly involves graph-based representations and iterative algorithms. Advances\n",
    "in algorithm design enable real-time and large-scale analysis. This pillar ensures that analytical solutions\n",
    "are not only theoretically sound but computationally viable.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar IX: Numerical Methods and Approximation\n",
    "\n",
    "- Real-world analytical problems rarely admit closed-form solutions. Numerical methods provide practical\n",
    "approximations while controlling error. Historically, these methods enabled scientific computation long\n",
    "before digital computers. In data analysis, numerical stability and precision directly affect reliability.\n",
    "\n",
    "- Floating-point arithmetic introduces subtle limitations that can distort results. Iterative solvers,\n",
    "approximation methods, and convergence criteria must be understood to avoid misleading outcomes.\n",
    "Historically, numerical analysis emphasized error bounds and conditioning. These concerns remain\n",
    "relevant despite increased computational power.\n",
    "\n",
    "- Modern analytics relies heavily on numerical linear algebra and optimization routines. Hardware\n",
    "acceleration amplifies both capability and risk. This pillar emphasizes vigilance in approximation and\n",
    "computation. It ensures that analytical conclusions are numerically defensible.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar X: Measurement, Scale, and Semantics\n",
    "\n",
    "- Measurement theory addresses how numerical representations relate to real-world phenomena. Not all\n",
    "numbers are interchangeable, and misuse can invalidate analysis.  Historically, this field supported\n",
    "scientific experimentation and instrumentation. In data analysis, it governs encoding and interpretation.\n",
    "\n",
    "- Variables may be nominal, ordinal, interval, or ratio in nature. Treating categorical codes as continuous\n",
    "quantities leads to invalid operations. Modern datasets often mix scales and units, increasing the risk of\n",
    "semantic error. Careful encoding preserves meaning.\n",
    "\n",
    "- This pillar reinforces the principle that data analysis is not purely mechanical. Understanding what\n",
    "numbers *represent* is as important as manipulating them. Measurement theory ensures that analytical\n",
    "operations respect real-world constraints and meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar XI: Causality and Structural Reasoning\n",
    "\n",
    "- Causality addresses questions of mechanism rather than association. Historically, causal reasoning was\n",
    "implicit in scientific experimentation but lacked formal tools. Twentieth-century advances introduced\n",
    "causal graphs and counterfactual frameworks. These developments transformed causal inference into a\n",
    "rigorous discipline.\n",
    "\n",
    "- In data analysis, causal reasoning distinguishes correlation from actionable insight. Policy evaluation,\n",
    "impact assessment, and experimental design all rely on causal thinking. Observational data pose\n",
    "particular challenges that require structural assumptions. Modern methods attempt to address these\n",
    "challenges explicitly.\n",
    "\n",
    "- As analytics increasingly informs decisions with real consequences, causality becomes indispensable.\n",
    "This pillar elevates analysis from description to explanation. It ensures that conclusions support sound\n",
    "intervention rather than coincidental association.\n",
    "\n",
    "---\n",
    "\n",
    "## Pillar XII: Validation, Robustness, and Epistemology\n",
    "\n",
    "- The final pillar concerns how analytical knowledge is justified and sustained. Epistemology asks what it\n",
    "means to know something and how certainty is established. In data analysis, validation techniques test\n",
    "whether results generalize beyond the data used to produce them. Robustness analysis examines sensitivity\n",
    "to assumptions.\n",
    "\n",
    "- Historically, scientific progress depended on reproducibility and peer scrutiny. Modern analytics faces\n",
    "similar demands, amplified by automation and scale. Governance, auditability, and transparency are now\n",
    "central concerns. Validation frameworks ensure accountability.\n",
    "\n",
    "- This pillar enforces intellectual discipline in analytical practice. It recognizes that models are\n",
    "approximations, not truths. By emphasizing uncertainty, assumptions, and reproducibility, it safeguards\n",
    "the credibility of data-driven decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- The Pillars of Data Analysis form a coherent intellectual framework rooted in centuries of mathematical\n",
    "and scientific thought. While tools and platforms evolve, these foundations remain stable and essential.\n",
    "Together, they support rigorous reasoning, responsible inference, and trustworthy analytics. Mastery of\n",
    "these pillars enables analysts to adapt to new technologies without sacrificing analytical integrity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Probability: \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Trial or experiment`: An action that results in a certain outcome with a certain likelihood\n",
    "\n",
    "- `Sample space`: This encompasses all potential outcomes of a given experiment\n",
    "\n",
    "- `Event`: This denotes a non-empty portion of the sample space\n",
    "\n",
    "- Probability is a measure of the likelihood of an event occurring when an experiment is conducted.\n",
    "\n",
    "- The probability of event A with one outcome is equal to the chance of event A divided by the chance of all possible events. \n",
    "\n",
    "- The value of probability ranges from 0 to 1, with the sample space embodying the complete set of potential outcomes, denoted as P(S) = 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Statistical Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two events are defined as independent if the occurrence of one event doesnâ€™t influence the likelihood of the other eventâ€™s occurrence. \n",
    "\n",
    "- Events `A` and `B` are independent precisely when `P(A and B) = P(A)P(B)`, where `P(A)` and `P(B)` are the respective probabilities of events `A` and `B` happening.\n",
    "\n",
    "- `Complementary event`: The complementary event to `A`, signified as `Aâ€™`, encompasses the probability of all potential outcomes in the sample space not included in `A`. \n",
    "\n",
    "- `Union and intersection`: The complementary event to `A`, signified as `Aâ€™`, encompasses the probability of all potential outcomes in the sample space not included in `A`. \n",
    "\n",
    "- `Mutually exclusive`: When two events have no shared outcomes, they are viewed as mutually exclusive. In other words, if `A` and `B` are mutually exclusive events, then `P(A âˆ© B) = 0`. \n",
    "\n",
    "- `Independent`: Two events are deemed independent when the occurrence of one doesnâ€™t impact the occurrence of the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  Discrete Random Variable\n",
    "\n",
    "- A discrete random variable refers to a variable that can assume a finite or countably infinite number \n",
    "of potential outcomes. \n",
    "\n",
    "- The probability distribution of a discrete random variable assigns a certain likelihood to each potential \n",
    "outcome the variable could adopt.\n",
    "\n",
    "- `Probability Mass Function (PMF)` - correlates each possible outcome of the variable to its likelihood of occurrence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PMF is bound by two principles:\n",
    "\n",
    "\n",
    " - It must be non-negative across all potential values of the random variable\n",
    "\n",
    " - The total sum of probabilities for all possible outcomes should equate to 1\n",
    "\n",
    " - The expected value of a discrete random variable offers an insight into its central tendency, computed as the probability-weighted average of its possible outcomes. \n",
    " \n",
    " - This expected value is signified as `E[X]`, with `X` representing the random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Probability Density Function` (PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A tool used to describe the distribution of a continuous random variable. \n",
    "\n",
    "- It can be used to calculate the probability of a value falling within a specific range. \n",
    "\n",
    "- It helps determine the chances of a continuous variable, `X`, having a value within the interval `[a, b]`, or in statistical terms,\n",
    " `P  ( A < X < B )` \n",
    "\n",
    "- For continuous variables, the probability of a single value occurring is always 0, which is in contrast to \n",
    "discrete variables that can assign non-0 probabilities to distinct values. \n",
    "\n",
    "- PDFs provide a way to estimate the likelihood of a value falling within a given range instead of a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Maximum-Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maximum likelihood is a statistical approach, that is used to estimate the parameters of a probability distribution. \n",
    "\n",
    "- The objective is to identify the parameter values that maximize the likelihood of observing the data, essentially determining the parameters most likely to have generated the data.\n",
    "\n",
    "- Suppose we have a random sample,  `X =  { X 1, â€¦ , X n}` , from a population with a probability distribution  `f( x | ðœ½ )`, where `Î¸` is a vector of parameters. \n",
    "\n",
    "- The likelihood of observing the sample, X, given the parameters, Î¸, is defined as the product of the individual probabilities of observing each data point:\n",
    "\n",
    "```\n",
    "\n",
    "        L( ðœ½  | X ) =  f ( X  | ðœ½ )\n",
    "\n",
    "```\n",
    "\n",
    "- In case of having independent and identically distributed observations, the likelihood function can be expressed as the product of the univariate density functions, each evaluated at the corresponding observation:\n",
    "\n",
    "```\n",
    "\n",
    "        L( ðœ½ | X) = f( X1| ðœ½ ) f( X2 | ðœ½ ) â€¦f( Xn| ðœ½ ) \n",
    "\n",
    "```\n",
    "\n",
    "- The `maximum likelihood estimate (MLE)` is the parameter vector value that offers the maximum value for the likelihood function across the parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In many cases, itâ€™s more convenient to employ the natural logarithm of the likelihood function, referred to as the `log-likelihood`. \n",
    "\n",
    "- The peak of the `log-likelihood` happens at the identical parameter vector value as the likelihood functionâ€™s maximum, and the conditions required for a maximum (or minimum) are acquired by equating the `log-likelihood` derivatives with respect to each parameter to 0. \n",
    "\n",
    "\n",
    "- If the `log-likelihood` is differentiable with respect to the parameters, these conditions result in a set of \n",
    "equations that can be solved numerically to derive the `MLE`. \n",
    "\n",
    "- `MLE` is often used to estimate the coefficients that define the relationship between input features and the target variable. \n",
    "\n",
    "- `MLE` helps find the values for the coefficients that maximize the likelihood of observing the given data under the assumed linear regression model, improving the accuracy of the predictions.\n",
    "\n",
    "- The `MLE`s of the parameters, `Î¸`, are the values that maximize the likelihood function. \n",
    "\n",
    "- In other words, the `MLE`s are the values of Î¸ that make the observed data, X, most probable.\n",
    "\n",
    "- To find the `MLE`s, we typically take the natural logarithm of the likelihood function, as it is often easier to work with the logarithm of a product than with the product itself:\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "            lnL( ðœ½ | X ) = lnf( X 1| ðœ½) + lnf( X 2| ðœ½ ) + â€¦ + lnf( X n| ðœ½ )\n",
    "\n",
    "```\n",
    "\n",
    "-  The `MLE`s are determined by equating the partial derivatives of the `log-likelihood` function with respect to each parameter to 0 and then solving these equations for the parameters\n",
    "\n",
    "- Once the `MLE`s have been found, they can be used to make predictions about the population based on the sample data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bayesian Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A statistical approach that involves updating our beliefs or probabilities about a quantity of interest based on new data. \n",
    "\n",
    "- In Bayesian estimation, we start with prior beliefs about the quantity of interest, which are expressed as a probability distribution. \n",
    "\n",
    "- These prior beliefs are updated as we collect new data. \n",
    "\n",
    "- The updated beliefs are represented as a posterior distribution. \n",
    "\n",
    "- The Bayesian framework provides a systematic way of updating prior beliefs with new data.\n",
    "\n",
    "- The choice of prior distribution is important, as it reflects our beliefs about the quantity of interest before collecting any data. \n",
    "\n",
    "- The prior distribution can be chosen based on prior knowledge or previous studies. \n",
    "\n",
    "- If no prior knowledge is available, a non-informative prior can be used, such as a uniform distribution.\n",
    "\n",
    "- Once the posterior distribution is calculated, it can be used to make predictions about the quantity of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In real-world business environments, one important task is to define the dataset from all possible sources of data, explore the gathered data to find the best method for preprocessing it, and ultimately decide on the ML and natural language models that fit the problem \n",
    "and the underlying data best. \n",
    "\n",
    "- Data exploration is an iterative process that involves visualizing and analyzing the data, looking for patterns and relationships, and identifying potential issues or outliers. \n",
    "\n",
    "- Data exploration is an important, initial step in the ML workflow that involves analyzing and understanding the data before building a ML model. \n",
    "\n",
    "- The goal of data exploration is to gain insights about the data, identify patterns, detect anomalies, and prepare the data for modeling. \n",
    "\n",
    "\n",
    "- In `Natural Language Processing (NLP)`, the data can be quite complex, as it often includes text and speech data that can be unstructured and difficult to analyze. \n",
    "\n",
    "- The first step of any NLP or ML solution starts with exploring the data to learn more about it, which helps us decide on our path to tackle the problem.\n",
    "\n",
    "- Preprocessing methods such as `tokenization`, `stemming`, and `lemmatization` can be employed. \n",
    "\n",
    "- It is important to note that employing effective preprocessing techniques can significantly enhance the performance and accuracy of ML models, making them more robust and reliable.\n",
    "\n",
    "- Once the data has been preprocessed and explored, we can start building our ML models. \n",
    "\n",
    "- There is no single magical solution that works for all ML problems, so itâ€™s important to carefully consider which models are best suited for the data and the problem at hand.\n",
    "\n",
    "\n",
    "- Data exploration helps in choosing the right ML algorithm and determining the best set of features to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### `Data visualization`: \n",
    "\n",
    "- Data visualization involves depicting data through graphical or pictorial formats. \n",
    "\n",
    "- It enables visual exploration of data, providing insights into its distribution, patterns, and relationships. \n",
    "\n",
    "- Widely employed techniques in data visualization encompass scatter plots, bar charts, heatmaps, box plots, and correlation matrices.\n",
    "\n",
    "#### `Data cleaning`: \n",
    "\n",
    "- Data cleaning is a step of preprocessing where we identify the errors, inconsistencies, and missing values and correct them. \n",
    "\n",
    "- It affects the final results of the model since ML models are sensitive to errors in the data. \n",
    "\n",
    "- Removing duplicates and filling in missing values are some of the common data cleaning techniques.\n",
    "\n",
    "#### `Feature engineering`: \n",
    "\n",
    "- Feature engineering plays a crucial role in optimizing the effectiveness of machine learning models by crafting new features from existing data. \n",
    "\n",
    "- This process involves not only identifying pertinent features but also transforming the existing ones and introducing novel features. \n",
    "\n",
    "- Various feature engineering techniques, including scaling, normalization, dimensionality reduction, and feature selection, contribute to refining the overall performance of the models.\n",
    "\n",
    "####  `Statistical analysis`: \n",
    "\n",
    "- Statistical analysis utilizes a range of statistical techniques to scrutinize data, revealing valuable insights into its inherent properties. \n",
    "\n",
    "- Essential statistical methods include hypothesis testing, regression analysis, and time series analysis, all of which contribute to a comprehensive understanding of the dataâ€™s characteristics.\n",
    "\n",
    "####  `Domain knowledge`: \n",
    "\n",
    "- Leveraging domain knowledge entails applying a pre-existing understanding of the data domain to extract insights and make informed decisions. \n",
    "\n",
    "- This knowledge proves valuable in recognizing pertinent features, interpreting results, and choosing the most suitable ML algorithm for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Involves recognizing and rectifying 0r eliminating errors, inconsistencies, and inaccuracies within a dataset. \n",
    "\n",
    "- A crucial phase in data preparation for ML significantly influences the accuracy and performance of a model, relying on the \n",
    "quality of the data used for training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dropping rows`: \n",
    "\n",
    "- Addressing missing data can involve a straightforward approach of discarding rows that contain such values. \n",
    "\n",
    "- Exercising caution is paramount when employing this method as excessive row removal may result in the loss of valuable data, impacting the \n",
    "overall accuracy of the model. \n",
    "\n",
    "- We usually use this method when we have a few rows in our dataset, and we have a few rows with missing values. In this case, removing a few rows can \n",
    "\n",
    "`Dropping columns`: \n",
    "\n",
    "- Another approach is to drop the columns that contain missing values. \n",
    "\n",
    "- This method can be effective if the missing values are concentrated in a few columns and if those columns are not important for the analysis. \n",
    "\n",
    "- It is better to perform some sort of correlation analysis to see the correlation of the values in these columns with the target class or value before dropping these columns.\n",
    "\n",
    "`Mean/Median/Mode Imputation`: \n",
    "\n",
    "- Mean, median, and mode imputation entail substituting missing values with the mean, median, or mode derived from the non-missing values within the corresponding column. \n",
    "\n",
    "- This method is easy to implement and can be effective when the missing values are few and randomly distributed. \n",
    "\n",
    "- However, it can also introduce bias and affect the variability of the data.\n",
    "\n",
    "\n",
    "`Regression Imputation`: \n",
    "\n",
    "- Regression imputation involves predicting the missing values based on the values of other variables in the dataset. \n",
    "\n",
    "- This method can be effective when the missing values are related to other variables in the dataset, but it requires a regression model to be built for each column with missing values.\n",
    "\n",
    "\n",
    "`Multiple Imputation`: \n",
    "\n",
    "- Multiple imputation encompasses generating multiple imputed datasets through statistical models, followed by amalgamating the outcomes to produce a conclusive dataset. \n",
    "\n",
    "- This approach proves efficacious, particularly when dealing with non-randomly distributed missing values and a substantial number of gaps in the dataset.\n",
    "\n",
    "\n",
    "`K-Nearest Neighbor Imputation`: \n",
    "\n",
    "- K-nearest neighbor imputation entails identifying the k-nearest data points to the missing value and utilizing their values to impute the absent value. \n",
    "\n",
    "- This method can be effective when the missing values are clustered together in the dataset. \n",
    "\n",
    "- In this approach, we can find the most similar records to the dataset to the record that has the missing value, and then use the mean of the values of those records for that specific record as the missed value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eliminating duplicates is a prevalent preprocessing measure thatâ€™s employed to cleanse datasets by detecting and removing identical records. \n",
    "\n",
    "- The occurrence of duplicate records may be attributed to factors such as data entry errors, system glitches, or data merging processes. \n",
    "\n",
    "- The presence of duplicates can skew models and yield inaccurate insights. \n",
    "\n",
    "- If two or more rows have the same values in all the columns, they are considered duplicates. \n",
    "\n",
    "- In some cases, it may be necessary to compare only a subset of columns if certain columns are more prone to duplicates.\n",
    "\n",
    "- Another method is to use a unique identifier column to identify duplicates\n",
    "\n",
    "- After identifying the duplicate records, the next step is to decide which records to keep and which ones to remove. \n",
    "\n",
    "- One approach is to keep the first occurrence of a duplicate record and remove all subsequent occurrences. \n",
    "\n",
    "- Another approach is to keep the record with the most complete information, or the record with the most recent timestamp.\n",
    "\n",
    "- Itâ€™s crucial to recognize that the removal of duplicates might lead to a reduction in dataset size, potentially affecting the performance of ML models. \n",
    "\n",
    "- Assessing the impact of duplicate removal on both the dataset and the ML model is essential. \n",
    "\n",
    "- In some cases, it may be necessary to keep duplicate records if they contain important information that cannot be obtained from other records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Standardizing and Transforming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This process involves scaling and normalizing the numerical features of the dataset to make them easier to interpret and compare. \n",
    "\n",
    "- The main objective of standardizing and transforming data is to enhance the accuracy and performance of a ML model by mitigating the influence of features with diverse scales and ranges. \n",
    "\n",
    "- A widely used method for standardizing data is referred to as `standardization` or `Z-score normalization`. This technique involves transforming each feature such that it has a mean of zero and a standard deviation of one. \n",
    "\n",
    "```\n",
    "\n",
    "        xâ€² =  ( x âˆ’ mean ( x ) ) / std ( x ) \n",
    "\n",
    "```\n",
    "\n",
    "- Here, `x` represents the feature, `mean(x)` denotes the mean of the feature, `std(x)` indicates the standard deviation of the feature, and `xâ€™` represents the new value assigned to the feature. \n",
    "\n",
    "- The range of each feature is adjusted to be centered around zero, which makes it easier to compare features and prevents features with large values from dominating the analysis.\n",
    "\n",
    "\n",
    "- Another technique for transforming data is `Min-Max Scaling`. This method rescales the data to a consistent range of values, commonly ranging between 0 and 1. The formula for min-max scaling is shown here:\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "        xâ€² =  ( x âˆ’ min ( x ) ) /  ( max ( x ) âˆ’ min ( x ) ) \n",
    "        \n",
    "\n",
    "```\n",
    "\n",
    "- In this equation, `x` represents the feature, `min(x)` signifies the minimum value of the feature, and `max(x)` denotes the maximum value of the feature. \n",
    "\n",
    "- Min-max scaling proves beneficial when the precise distribution of the data is not crucial, but there is a need to standardize the data for meaningful comparisons across different features.\n",
    "\n",
    "- Transforming data can also involve changing the distribution of the data. \n",
    "\n",
    "- A frequently applied transformation is the `Log Transformation`, which is employed to alleviate the influence of outliers and skewness within the data. This transformation involves taking the logarithm of the feature values which can help to normalize the distribution and reduce the influence of extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Outliers are data points that markedly deviate from the rest of the observations in a dataset. \n",
    "\n",
    "- Their occurrence may stem from factors such as measurement errors, data corruption, or authentic extreme values. \n",
    "\n",
    "- The presence of outliers can wield a substantial influence on the outcomes of ML models, introducing distortion to the data and disrupting the relationships between variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Removing Outliers`: \n",
    "\n",
    "- One straightforward approach involves eliminating observations identified as outliers from the dataset. \n",
    "\n",
    "- Exercising caution is paramount when adopting this method as excessive removal of observations may result in the loss of valuable information and potentially introduce bias to the analysis results.\n",
    "\n",
    "\n",
    "#### `Transforming Data`: \n",
    "\n",
    "- Applying mathematical functions such as logarithms or square roots to transform the data can mitigate the influence of outliers. \n",
    "\n",
    "- For instance, taking the logarithm of a variable can alleviate the impact of extreme values, given the slower rate of increase in the \n",
    "logarithmic scale compared to the original values.\n",
    "\n",
    "\n",
    "#### `Winsorizing`:\n",
    "\n",
    "- Winsorizing is a technique that entails substituting extreme values with the nearest highest or lowest value in the dataset. \n",
    "\n",
    "- Employing this method aids in maintaining the sample size and overall distribution of the data.\n",
    "\n",
    "#### `Imputing Values`: \n",
    "\n",
    "- Imputation involves replacing missing or extreme values with estimated values derived from the remaining observations in the dataset. \n",
    "\n",
    "- For instance, substituting extreme values with the median or mean of the remaining observations is a common imputation technique.\n",
    "\n",
    "#### `Robust Statistical Methods`: \n",
    "\n",
    "- Robust statistical methods exhibit lower sensitivity to outliers, leading to more accurate results even in the presence of such extreme values. \n",
    "\n",
    "- For instance, opting for the median instead of the mean can effectively diminish the influence of outliers on the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Error Correction"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
